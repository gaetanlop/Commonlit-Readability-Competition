{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Deberta Large Training",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWC7Veto88U2"
      },
      "source": [
        "### This notebook is just an example of the training of one of my bert models. It uses decay learning rate instead of layer wise learning rate (mostly used in other bert models). I tried a lot of different heads for deberta but the best performing one seems to be a Mean Pooling head."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_oLRpv3Yz8M"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBICDyP785uK"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6pW6udV84pM"
      },
      "source": [
        "pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7LZMQyiY1I3"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import os\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm_notebook\n",
        "\n",
        "from transformers import AutoConfig, AutoTokenizer, AutoModel\n",
        "import transformers\n",
        "from transformers import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
        "\n",
        "import torch.nn.init as init\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Parameter\n",
        "from torch.autograd.function import InplaceFunction\n",
        "import math\n",
        "\n",
        "from torch.utils.data import Sampler, Dataset, DataLoader\n",
        "import random\n",
        "\n",
        "from more_itertools import chunked, flatten"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4k-dmgh_Y4BO",
        "outputId": "c643e4f9-7165-4938-aaec-86753a5cbec3"
      },
      "source": [
        "%cd drive/MyDrive/CommonLit"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/CommonLit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJbmbN0tpgqQ"
      },
      "source": [
        "# Get folds"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akuiMAmCaEaF"
      },
      "source": [
        "df = pd.read_csv(\"train_folds.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wc6U6Of0aiB2"
      },
      "source": [
        "# Seed Everything"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xMz3-kDaTkA"
      },
      "source": [
        "def seed_everything(seed=12):\n",
        "\n",
        "  \"\"\"\n",
        "  Try to make the result as reproducible as possible.\n",
        "  \"\"\"\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "seed_everything(seed=12)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPHzEnpEdbq0"
      },
      "source": [
        "# Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TusK1jchaxBH"
      },
      "source": [
        "MAX_LEN = 256\n",
        "EPOCHS = 3\n",
        "DEBERTA_PATH = \"microsoft/deberta-large\"\n",
        "TRAIN_BATCH_SIZE = 4\n",
        "VALID_BATCH_SIZE = 8\n",
        "LEARNING_RATE = 2e-5\n",
        "TOKENIZER = transformers.AutoTokenizer.from_pretrained(DEBERTA_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VvkXeaejg3Y"
      },
      "source": [
        "# Evaluation Scheduler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJ8mYXGCjizi"
      },
      "source": [
        "def evaluate(EVAL_STEPS,valid_interval, valid_loss, train_loss, final_train_loss, index, best_loss, epoch):\n",
        "\n",
        "  \"\"\"\n",
        "  Just a function to follow the training process and save the best performing one.\n",
        "\n",
        "  Args:\n",
        "    EVAL_STEPS (list) : each validation rmse corresponds to a specific valid_interval\n",
        "    valid_interval (int) : number of intervals between which we validate the model and save the best performing one\n",
        "    valid_loss (float) : validation loss of the current iteration\n",
        "    train_loss (float) : training loss\n",
        "    final_train_loss (float) : average of the current loss over all the iterations\n",
        "    index (int) : number of iterations\n",
        "    best_loss (float) : best validation loss\n",
        "    epoch (int) : epoch number\n",
        "\n",
        "  \"\"\"\n",
        "    \n",
        "        print(f\"Epoch:{epoch}| Batch {index} | Train Loss:{train_loss.avg()} | Validation loss:{valid_loss}\")\n",
        "        if (valid_loss < best_loss):\n",
        "            \n",
        "            for rmse, steps in EVAL_STEPS:\n",
        "                if valid_loss > rmse:\n",
        "                    valid_interval = steps\n",
        "                    break\n",
        "              \n",
        "            print(f\"Validation loss decreased from {best_loss} to {valid_loss}.\")\n",
        "            final_train_loss = train_loss.avg()\n",
        "            best_loss = valid_loss\n",
        "            torch.save(model.state_dict(),f'Training/Models/Deberta/model{fold}.bin')\n",
        "\n",
        "            \n",
        "        return valid_interval, best_loss, final_train_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwSb-EcbfnP9"
      },
      "source": [
        "# Training Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8-mUVIHflkO"
      },
      "source": [
        "def train_fn(train_dataloader, valid_dataloader, model, optimizer, device, scheduler):\n",
        "\n",
        "  \"\"\"\n",
        "  Pytorch function to train a model and validate it. We do not validate the model at the end of each epoch but at the end of \n",
        "  a specific number of iterations. (cf. Readme file of github)\n",
        "\n",
        "  Args : \n",
        "      train_dataloader (pytorch dataloader) : training dataloader\n",
        "      valid_dataloader (pytorch dataloader) : validation dataloader\n",
        "      model (nn.Module) : the model that you want to train\n",
        "      optimizer : optimizer\n",
        "      device : cpu or gpu\n",
        "      scheduler : scheduler\n",
        "  \"\"\"\n",
        "    \n",
        "\n",
        "    EVAL_STEPS = [(0.50,400),(0.49,400), (0.48, 200), (-1., 200)]\n",
        "    valid_interval = EVAL_STEPS[0][1]\n",
        "    best_loss = np.inf\n",
        "    final_train_loss = None\n",
        "    accumulation_steps = 4\n",
        "    lr_schedule = [2e-5, 5e-6, 2e-6]\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "      \n",
        "        train_loss = AvgCounter()\n",
        "        lr = lr_schedule[epoch]\n",
        "        optimizer = scheduler(optimizer,lr)\n",
        "\n",
        "        for index, d in tqdm_notebook(enumerate(train_dataloader), total=len(train_dataloader)):\n",
        "  \n",
        "            ids = d[\"ids\"]\n",
        "            mask = d[\"mask\"]\n",
        "            targets = d[\"targets\"]\n",
        "\n",
        "            ids = ids.to(device, dtype=torch.long)\n",
        "            mask = mask.to(device, dtype=torch.long)\n",
        "            targets = targets.to(device, dtype=torch.float)\n",
        "\n",
        "            model.train()\n",
        "            loss, outputs = model(ids=ids, mask=mask, loss_fn = loss_fn, targets = targets)\n",
        "            \n",
        "            train_loss.update(loss.item(), len(d))\n",
        "            loss = loss / accumulation_steps \n",
        "            loss.backward()\n",
        "\n",
        "            if index % accumulation_steps == 0:             \n",
        "                optimizer.step() \n",
        "                # scheduler.step()                           \n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            if (index % valid_interval == 0) | ((len(train_dataloader)-index) == 1):\n",
        "\n",
        "                valid_loss = eval_fn(valid_dataloader,model,device)\n",
        "                \n",
        "                valid_interval, best_loss, final_train_loss = evaluate(EVAL_STEPS,valid_interval, valid_loss,train_loss, final_train_loss, index, best_loss, epoch )\n",
        "            \n",
        "    return final_train_loss, best_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2PX80WQjqTb"
      },
      "source": [
        "# Evaluation Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xgwc97gdjsBK"
      },
      "source": [
        "def eval_fn(data_loader, model, device):\n",
        "  \"\"\"\n",
        "  Evaluate the performance of our model.\n",
        "\n",
        "  -------------------\n",
        "  Args:\n",
        "      dataloader (pytorch dataloader) : validation dataloader\n",
        "      model (nn.Module) : the model you are training\n",
        "      device : cpu or gpu\n",
        "  -------------------\n",
        "  Returns:\n",
        "      Validation loss\n",
        "  \"\"\"\n",
        "    model.eval()\n",
        "    valid_loss = AvgCounter()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for bi, d in enumerate(data_loader):\n",
        "            ids = d[\"ids\"]\n",
        "            mask = d[\"mask\"]\n",
        "            targets = d[\"targets\"]\n",
        "\n",
        "            ids = ids.to(device, dtype=torch.long)\n",
        "            mask = mask.to(device, dtype=torch.long)\n",
        "            targets = targets.to(device, dtype=torch.float)\n",
        "\n",
        "            loss, outputs = model(ids=ids, mask=mask, loss_fn = loss_fn, targets = targets)\n",
        "            \n",
        "            valid_loss.update(loss.item(), len(d))\n",
        "            \n",
        "    return valid_loss.avg()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HMMTxqWff7x"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rklfe2j9dewv"
      },
      "source": [
        "class DebertaDataset:\n",
        "  \"\"\"\n",
        "  Simple pytorch dataset class using deberta tokenizer from hugging face\n",
        "  \"\"\"\n",
        "    def __init__(self,df):\n",
        "        self.excerpt = df.excerpt.values\n",
        "        self.target = df.target.values\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.excerpt)\n",
        "    \n",
        "    def __getitem__(self,item):\n",
        "        excerpt = str(self.excerpt[item])\n",
        "        excerpt = \" \".join(excerpt.split())\n",
        "        inputs = TOKENIZER(excerpt, add_special_tokens = True, max_length = MAX_LEN, padding=True, truncation=True)\n",
        "        \n",
        "        ids = inputs[\"input_ids\"]\n",
        "        mask = inputs[\"attention_mask\"]\n",
        "        token_type_ids = inputs[\"token_type_ids\"]\n",
        "        \n",
        "        padding_len = MAX_LEN-len(ids)\n",
        "        ids = ids+([0]*padding_len)\n",
        "        mask = mask+([0]*padding_len)\n",
        "        token_type_ids = token_type_ids+([0]*padding_len)\n",
        " \n",
        "        return {\"ids\": torch.tensor(ids, dtype=torch.long),\n",
        "            \"mask\": torch.tensor(mask, dtype=torch.long),\n",
        "            \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n",
        "            \"targets\": torch.tensor(self.target[item], dtype=torch.float)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Urdqz9g8fhwW"
      },
      "source": [
        "# Model Building"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLXbHlx_fi0b"
      },
      "source": [
        "class DebertaModel(nn.Module):\n",
        "\n",
        "  \"\"\"\n",
        "    Simple Deberta Model with two possible heads: Mean pooling or attention pooling.\n",
        "  \"\"\"\n",
        "    \n",
        "    def __init__(self, model_type=\"mean\"):\n",
        "        super(DebertaModel,self).__init__()\n",
        "\n",
        "        self.model_type = model_type\n",
        "        \n",
        "        self.config = AutoConfig.from_pretrained(DEBERTA_PATH)\n",
        "        self.config.update({\"output_hidden_states\":True, \n",
        "                       \"hidden_dropout_prob\": 0.0,\n",
        "                       \"layer_norm_eps\": 1e-7})   \n",
        "        \n",
        "        self.deberta = transformers.AutoModel.from_pretrained(DEBERTA_PATH, config=self.config)\n",
        "\n",
        "        if model_type == \"attention\":\n",
        "            \n",
        "            self.attention = nn.Sequential(            \n",
        "            nn.Linear(1024, 1024),            \n",
        "            nn.Tanh(),                       \n",
        "            nn.Linear(1024, 1),\n",
        "            nn.Softmax(dim=1)\n",
        "            )   \n",
        "\n",
        "            self.linear = (nn.Linear(1024, 1))\n",
        "                           \n",
        "        elif model_type == \"mean\":\n",
        "        \n",
        "            self.layer_norm1 = nn.LayerNorm(1024)\n",
        "            self.linear1 = nn.Linear(1024, 768)\n",
        "            self.linear2 = nn.Linear(768, 1)\n",
        "            self.layer_norm2 = nn.LayerNorm(768)\n",
        "\n",
        "    def freeze(self):\n",
        "        for child in self.roberta.children():\n",
        "            for param in child.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def unfreeze(self):\n",
        "        for child in self.roberta.children():\n",
        "            for param in child.parameters():\n",
        "                param.requires_grad = True\n",
        "        \n",
        "    def forward(self, ids, mask, loss_fn = None, targets = None):\n",
        "\n",
        "        if self.model_type == \"mean\":\n",
        "\n",
        "            outputs = self.deberta(ids, mask)\n",
        "            last_hidden_state = outputs[0]\n",
        "            input_mask_expanded = mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
        "            sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
        "            sum_mask = input_mask_expanded.sum(1)\n",
        "            sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
        "            mean_embeddings = sum_embeddings / sum_mask\n",
        "            norm_mean_embeddings = self.layer_norm1(mean_embeddings)\n",
        "            logits = self.linear1(norm_mean_embeddings)\n",
        "            logits = self.linear2(self.layer_norm2(logits))\n",
        "\n",
        "        elif self.model_type==\"attention\":\n",
        "\n",
        "            roberta_output = self.deberta(input_ids=ids,\n",
        "                                  attention_mask=mask)        \n",
        "            last_layer_hidden_states = roberta_output.last_hidden_state\n",
        "            weights = self.attention(last_layer_hidden_states)\n",
        "            context_vector = torch.sum(weights * last_layer_hidden_states, dim=1) \n",
        "            logits = self.linear(context_vector)\n",
        "\n",
        "        if targets is not None:\n",
        "\n",
        "            loss = torch.sqrt(loss_fn(logits.view(-1),targets.view(-1)))\n",
        "            return loss, logits\n",
        "\n",
        "        else:\n",
        "\n",
        "            return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhLjqRAAjvzG"
      },
      "source": [
        "# AvgCounter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbcrRmzejxPF"
      },
      "source": [
        "class AvgCounter:\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "        \n",
        "    def update(self, loss, n_samples):\n",
        "        self.loss += loss * n_samples\n",
        "        self.n_samples += n_samples\n",
        "        \n",
        "    def avg(self):\n",
        "        return self.loss / self.n_samples\n",
        "    \n",
        "    def reset(self):\n",
        "        self.loss = 0\n",
        "        self.n_samples = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2zQuwbzDCoC"
      },
      "source": [
        "# Useful Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kPx4fK5DEOy"
      },
      "source": [
        "# create pytorch dataloader\n",
        "def create_dataloader(df, fold):\n",
        "\n",
        "  \"\"\"\n",
        "  Create the training and validation dataloader for a specific fold number.\n",
        "  The training set is composed of the training set from Commonlit and also external data.\n",
        "\n",
        "  -----------------------\n",
        "  Args:\n",
        "      df (dataframe) : dataframe with all sentences (commonlit data)\n",
        "      fold (int) : fold number\n",
        "  -----------------------\n",
        "  Returns:\n",
        "      training dataloader and validation dataloader\n",
        "\n",
        "  \"\"\"\n",
        "    \n",
        "    train = df[df.kfold!=fold].reset_index(drop=True)\n",
        "    print(train.shape)\n",
        "    x = pd.read_csv(f\"External Data/pseudo_labels_fold_queries_{fold}.csv\")\n",
        "    x = x[x.to_keep==1]\n",
        "    x = x[[\"sentences\", \"predictions\", \"stdev\"]]\n",
        "    x.columns = [\"excerpt\", \"target\", \"standard_error\"]\n",
        "    x[\"kfold\"] = None\n",
        "    train = pd.concat([train, x], axis = 0)\n",
        "    print(train.shape)\n",
        "    valid = df[df.kfold==fold].reset_index(drop=True)\n",
        "\n",
        "    train_dataset = DebertaDataset(train)\n",
        "    valid_dataset = DebertaDataset(valid)\n",
        "\n",
        "    sampler = torch.utils.data.RandomSampler(train_dataset)\n",
        "\n",
        "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size= TRAIN_BATCH_SIZE, sampler = sampler  )\n",
        "    valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size= VALID_BATCH_SIZE)\n",
        "\n",
        "    return train_dataloader, valid_dataloader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zJZyIA_j7WK"
      },
      "source": [
        "def create_model(device):\n",
        "\n",
        "    \"\"\"\n",
        "    Create the model and put it on a specific device\n",
        "    \"\"\"\n",
        "\n",
        "    model = DebertaModel().to(device)\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cMTBbBVj7lF"
      },
      "source": [
        "# create the optimizer\n",
        "def create_optimizer(model):\n",
        "  \"\"\"\n",
        "  Implementation of layer wize learning rate for a deberta model.\n",
        "\n",
        "  Returns:\n",
        "      AdamW optimizer with a specific learning rate for each layer of the deberta model\n",
        "  \"\"\"\n",
        "    named_parameters = list(model.named_parameters()) \n",
        "    no_decay = ['bias', 'gamma', 'beta']   \n",
        "    \n",
        "    parameters = []\n",
        "    lr = 3e-5\n",
        "    regressor_lr = 2e-5\n",
        "    for layer in range(23,-1,-1):\n",
        "        layer_params = {\n",
        "          'params': [\n",
        "                      p for n,p in model.named_parameters() if not any(nd in n for nd in no_decay) \\\n",
        "                      and (f'encoder.layer.{layer}.' in n)\n",
        "                      ],\n",
        "          'lr': lr\n",
        "      }\n",
        "        parameters.append(layer_params)\n",
        "\n",
        "        lr *= 0.975\n",
        "\n",
        "    regressor_params = {\n",
        "      'params': [p for n,p in model.named_parameters() if \"deberta\" not in n],\n",
        "      'lr': regressor_lr\n",
        "    }\n",
        "\n",
        "    parameters.append(regressor_params)\n",
        "\n",
        "    regressor_params = {\n",
        "      'params': [\n",
        "                      p for n,p in model.named_parameters() if not any(nd in n for nd in no_decay) \\\n",
        "                      and (f'deberta.embeddings' in n)\n",
        "                      ],\n",
        "      'lr': regressor_lr\n",
        "    }\n",
        "    parameters.append(regressor_params)\n",
        "\n",
        "    return AdamW(parameters)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8esDnOMlkBwM"
      },
      "source": [
        "# create scheduler\n",
        "def create_scheduler(optimizer, num_warmup_steps, num_train_steps, scheduler_name = \"get_cosine_schedule_with_warmup\" ):\n",
        "\n",
        "    if scheduler_name == \"get_linear_schedule_with_warmup\":\n",
        "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_train_steps)\n",
        "        \n",
        "    elif scheduler_name == \"get_cosine_schedule_with_warmup\":\n",
        "        scheduler = get_cosine_schedule_with_warmup(optimizer,num_training_steps=num_train_steps,num_warmup_steps=50) \n",
        "        \n",
        "    else:\n",
        "        raise Exception(f\"Unknown scheduler: {scheduler_name}\")\n",
        "\n",
        "    return scheduler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QllXh8LK7VN"
      },
      "source": [
        "def scheduler(optimizer,lr):\n",
        "  \"\"\"\n",
        "  Fast and simple implementation of decay learning rate.\n",
        "  At each epoch we change the learning rate. the learning rate is an hyper parameter that we need to tune.\n",
        "  \"\"\"\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "    return optimizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcpplvcffxN7"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-LANPCBFiTx"
      },
      "source": [
        "loss_fn=nn.MSELoss()\n",
        "loss=defaultdict(list)\n",
        "results_val = {}\n",
        "results_train = {}\n",
        "for fold in range(5):\n",
        "\n",
        "    seed_everything(42)\n",
        "    \n",
        "    device = torch.device(\"cuda\")\n",
        "    model = create_model(device)\n",
        "\n",
        "    print(\"################################\")\n",
        "    print(f\"Training Fold {fold}\")\n",
        "    print(\"################################\")\n",
        "\n",
        "    train_dataloader, valid_dataloader = create_dataloader(df, fold)\n",
        "    num_train_steps = len(train_dataloader) * EPOCHS\n",
        "\n",
        "    # not useful as we are using learning rate decay this time\n",
        "    optimizer = create_optimizer(model)\n",
        "    # scheduler = create_scheduler(optimizer, num_warmup_steps = 0, num_train_steps = num_train_steps )\n",
        "\n",
        "    seed_everything(42)\n",
        "    \n",
        "    results_train[fold], results_val[fold] = train_fn(train_dataloader,valid_dataloader, model, optimizer, device, scheduler)\n",
        "\n",
        "print(\"################################\")\n",
        "print(\"RESULTS\")\n",
        "print(\"################################\")\n",
        "cv_val = np.mean([results_val[i] for i in range(5)])\n",
        "cv_train = np.mean([results_train[i] for i in range(5)])\n",
        "print(f\"Results of cross validation for seed 12: Train : {cv_train}, Val : {cv_val}\") # I always used seed 12"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}