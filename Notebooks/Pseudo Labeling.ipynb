{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Pseudo Labeling.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOAA5As82ibCpqy+/Isj/CE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"-4VbWdJy7fP-"},"source":["## The following code aims at finding the most similar external sentences with respect to the training set."]},{"cell_type":"markdown","metadata":{"id":"ZNQbVA8O8_5s"},"source":["# Imports"]},{"cell_type":"code","metadata":{"id":"6H06YP_t2cRX"},"source":["!pip install transformers\n","!pip install sentence-transformers"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tHMKujwD9FFa"},"source":["from transformers import AutoConfig, AutoModel, AutoTokenizer\n","from sentence_transformers import SentenceTransformer, util\n","import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import os\n","import time\n","import transformers"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hN76AePVzgYK"},"source":["# Configuration"]},{"cell_type":"code","metadata":{"id":"WRpyVode2dzg"},"source":["MAX_LEN = 256\n","ELECTRA_PATH = \"google/electra-large-discriminator\"\n","ROBERTA_LARGE_PATH = \"roberta-large\"\n","ROBERTA_BASE_PATH = \"roberta-base\"\n","TOKENIZER_ROBERTA = transformers.AutoTokenizer.from_pretrained(ROBERTA_LARGE_PATH)\n","TOKENIZER_ELECTRA = AutoTokenizer.from_pretrained(ELECTRA_PATH)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tigHyOo89mFg"},"source":["# Set up google drive"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_UXd8Isb9ijy","executionInfo":{"status":"ok","timestamp":1631838961098,"user_tz":-120,"elapsed":18165,"user":{"displayName":"Gaetan Lopez","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13795704448222064979"}},"outputId":"a2aca1c0-9394-442e-99fc-2152d8db9184"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1zsOIoPt9pnI","executionInfo":{"status":"ok","timestamp":1631838961959,"user_tz":-120,"elapsed":865,"user":{"displayName":"Gaetan Lopez","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13795704448222064979"}},"outputId":"baf61dec-a7d6-41a5-ee1e-578cc0421642"},"source":["%cd \"/content/drive/My Drive/CommonLit/External Data\""],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/My Drive/CommonLit/External Data\n"]}]},{"cell_type":"markdown","metadata":{"id":"3T0A4hvH-PAd"},"source":["# Function to find similar texts"]},{"cell_type":"code","metadata":{"id":"uqMpUmU495y-"},"source":["def top_k_most_similar_texts(queries, corpus_text, corpus_embedding, top_k, model_name = 'paraphrase-TinyBERT-L6-v2'):\n","\n","  \"\"\"\n","  Creates the embedding for the training set that you want to compare to external data and returns the 5 sentences \n","  from the external data that are the most similar to each training sentences.\n","\n","  Args:\n","      queries : list of training sentences\n","      corpus_text : list of external sentences\n","      corpus_embedding : list of the embeddings of external sentences\n","      top_k : the number of external sentences that you want to return from the external sentences and for each training sentences.\n","              For example: if top_k = 5 then if you have 100 training sentences it will return 500 sentences\n","      model_name : model used to encode the training sentences. Needs to be the same model used to encode external data\n","\n","  \"\"\"\n","  model = SentenceTransformer(model_name)\n","  print(\"Start encoding queries\")\n","  queries_embedding = model.encode(queries, convert_to_tensor=True)\n","  print(\"Start to select sentences\")\n","  selected_sentences = util.semantic_search(queries_embedding, corpus_embedding, top_k= top_k)\n","\n","  selected = []\n","  for sentence in selected_sentences:\n","    sents = [corpus_text[s['corpus_id']] for s in sentence]\n","    selected.append(sents)\n","\n","  return selected"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5ZlX-HdR9_kb"},"source":["def load_external_text_embedding(datasets):\n","\n","  \"\"\"\n","  Put all external sentences and all external embeddings into one list.\n","\n","  Args:\n","      datasets (str) : the name of the dataset\n","\n","  Example usage:\n","\n","      embeddings, sentences = load_external_text_embedding([\"wiki\", \"simple_wiki\")\n","      Returns the embeddings and the sentences of simple_wiki and wikipedia datasets into one list.\n","  \"\"\"\n","  \n","  embeddings = []\n","  sentences = []\n","  for data in datasets:\n","    print(f\"Creating {data}\")\n","    embed_dir = os.path.join(\"encoded_sentences\",data + \".pt\") \n","    sentences_dir = os.path.join(\"preprocessed_data\",data + \".csv\") \n","    encoded = torch.load(embed_dir)\n","    sentence = pd.read_csv(sentences_dir)\n","    embeddings.extend(encoded)\n","    sentences.extend(sentence.text.values)\n","\n","  assert len(embeddings)==len(sentences)\n","\n","  return embeddings, sentences\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Kuko2-2FBjyH"},"source":["def zip_external_data_scores(selected_sentences, stdev, targets):\n","\n","  zipped = []\n","  for index, sentence in enumerate(selected_sentences):\n","    data_scores = [(sent, targets[index], stdev[index]) for sent in sentence]\n","    zipped.extend(data_scores)\n","\n","  return zipped"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8ccV3PVXZayB"},"source":["def create_folds(fold):\n","\n","  \"\"\"\n","  Returns the sentences, the targets and the standard deviations of the sentences which does not belong to the fold number specified in the parameter.\n","\n","  Args:\n","      fold (int) : fold number of the future external data\n","  \"\"\"\n","  \n","  df = pd.read_csv(\"train_folds.csv\")\n","  train =  df[df.kfold!=fold] \n","  targets = [float(t) for t in train.target.values]\n","  queries = [str(t) for t in train.excerpt.values]\n","  stdev = [float(t) for t in train.standard_error.values]\n","\n","  return queries, targets, stdev"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ovtfPqq-bNkg"},"source":["def run(datasets, top_k = 5, folds = 5):\n","  \"\"\"\n","  Takes a list of datasets and returns the top k most similar sentences with their targets and standard deviations\n","\n","  Args:\n","      datasets (list) : list of string for the name of the datasets\n","      top_k (int) : number of similar sentences for each training samples\n","      fodls (int) : number of folds\n","  \"\"\"\n","  for fold in range(folds):\n","    start = time.time()\n","\n","    print(\"Creating folds\")\n","    queries, targets, stdev = create_folds(fold)\n","    print(f'Time: {time.time() - start}')\n","    print(\"Loading External Data\")\n","    embeddings, sentences = load_external_text_embedding(datasets)\n","    print(f'Time: {time.time() - start}')\n","    print(\"Encoding queries and finding the top k most similar texts\")\n","\n","    selected_sentences = top_k_most_similar_texts(queries, sentences, embeddings, top_k = top_k)\n","    print(f'Time: {time.time() - start}')\n","    print(\"Zip chosen External Data with most similar sentences\")\n","\n","    zipped = pd.DataFrame(zip_external_data_scores(selected_sentences, stdev, targets))\n","    print(f'Time: {time.time() - start}')\n","    print(\"Saving\")\n","    zipped.columns = [\"sentences\",\"targets\", \"stdev\"]\n","    zipped.to_csv(f\"queries_{fold}.csv\")\n","\n","  return zipped"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZSw5ylFozpOA"},"source":["# datasets = [\"wikipedia\", \"simple_wikipedia\", \"onestop\", \"cbt\"]\n","datasets = [\"simple_wikipedia\", \"onestop\", \"wikipedia\", \"cbt\"]\n","target = run(datasets)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7KhfOrXTd7_u"},"source":["# Pseudo labeling"]},{"cell_type":"markdown","metadata":{"id":"30sMGXha5fow"},"source":["### The following code aims at creating the pseudo labels for the external data. At the end, we also filter another time this external data so that it is closer to our training set.\n"]},{"cell_type":"markdown","metadata":{"id":"FFL8nZ9Jeo6S"},"source":["## Dataset"]},{"cell_type":"code","metadata":{"id":"oApVO2x_d-L_"},"source":["class RobertaDataset:\n","    def __init__(self,df):\n","        self.excerpt = df.sentences.values\n","        self.target = df.targets.values\n","\n","    def __len__(self):\n","        return len(self.excerpt)\n","    \n","    def __getitem__(self,item):\n","        excerpt = str(self.excerpt[item])\n","        excerpt = \" \".join(excerpt.split())\n","        inputs = TOKENIZER_ROBERTA(excerpt, add_special_tokens=True, max_length=MAX_LEN, padding=True, truncation=True)\n","        \n","        ids = inputs[\"input_ids\"]\n","        mask = inputs[\"attention_mask\"]\n","        \n","        padding_len = MAX_LEN-len(ids)\n","        ids = ids+([0]*padding_len)\n","        mask = mask+([0]*padding_len)\n"," \n","        return {\"ids\": torch.tensor(ids, dtype=torch.long),\n","            \"mask\": torch.tensor(mask, dtype=torch.long),\n","            \"targets\": torch.tensor(self.target[item], dtype=torch.float)}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RQIN2Fn1eBXm"},"source":["class ElectraDataset:\n","    def __init__(self,df):\n","        self.excerpt = df.sentences.values\n","        self.target = df.targets.values\n","\n","    def __len__(self):\n","        return len(self.excerpt)\n","    \n","    def __getitem__(self,item):\n","        excerpt = str(self.excerpt[item])\n","        excerpt = \" \".join(excerpt.split())\n","        inputs = TOKENIZER_ELECTRA(excerpt, add_special_tokens=True, max_length=MAX_LEN, padding=True, truncation=True)\n","        \n","        ids = inputs[\"input_ids\"]\n","        mask = inputs[\"attention_mask\"]\n","        token_type_ids = inputs[\"token_type_ids\"]\n","        \n","        padding_len = MAX_LEN-len(ids)\n","        ids = ids+([0]*padding_len)\n","        mask = mask+([0]*padding_len)\n","        token_type_ids = token_type_ids+([0]*padding_len)\n"," \n","        return {\"ids\": torch.tensor(ids, dtype=torch.long),\n","            \"mask\": torch.tensor(mask, dtype=torch.long),\n","            \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n","            \"targets\": torch.tensor(self.target[item], dtype=torch.float)}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qS_wZTo4eqqE"},"source":["## Models"]},{"cell_type":"code","metadata":{"id":"yGa7KF3veB2H"},"source":["class RobertaBaseAttention(nn.Module):\n","    \n","    def __init__(self, model_type=\"attention\"):\n","        super(RobertaBaseAttention,self).__init__()\n","\n","        self.model_type = model_type\n","        \n","        self.config = AutoConfig.from_pretrained(ROBERTA_BASE_PATH)\n","        self.config.update({\"output_hidden_states\":True, \n","                       \"hidden_dropout_prob\": 0.0,\n","                       \"layer_norm_eps\": 1e-7})   \n","        self.roberta = transformers.AutoModel.from_pretrained(ROBERTA_BASE_PATH, config=self.config)\n","\n","        if model_type == \"attention\":\n","            \n","            self.attention = nn.Sequential(            \n","            nn.Linear(768, 256),            \n","            nn.Tanh(),                       \n","            nn.Linear(256, 1),\n","            nn.Softmax(dim=1)\n","            )   \n","\n","            self.layer_norm1 = nn.LayerNorm(768)\n","            self.linear1 = nn.Linear(768, 256)\n","            self.linear2 = nn.Linear(256, 1)\n","            self.layer_norm2 = nn.LayerNorm(256)\n","                           \n","        elif model_type == \"mean\":\n","        \n","            self.layer_norm1 = nn.LayerNorm(1024)\n","            self.linear1 = nn.Linear(1024, 256)\n","            self.linear2 = nn.Linear(256, 1)\n","            self.layer_norm2 = nn.LayerNorm(256)\n","\n","    def freeze(self):\n","        for child in self.roberta.children():\n","            for param in child.parameters():\n","                param.requires_grad = False\n","\n","    def unfreeze(self):\n","        for child in self.roberta.children():\n","            for param in child.parameters():\n","                param.requires_grad = True\n","        \n","    def forward(self, ids, mask, loss_fn = None, targets = None):\n","\n","        if self.model_type == \"mean\":\n","\n","            outputs = self.roberta(ids, mask)\n","            last_hidden_state = outputs[0]\n","            input_mask_expanded = mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n","            sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n","            sum_mask = input_mask_expanded.sum(1)\n","            sum_mask = torch.clamp(sum_mask, min=1e-9)\n","            mean_embeddings = sum_embeddings / sum_mask\n","            norm_mean_embeddings = self.layer_norm1(mean_embeddings)\n","            logits = self.linear1(norm_mean_embeddings)\n","            logits = self.linear2(self.layer_norm2(logits))\n","\n","        elif self.model_type==\"attention\":\n","\n","            roberta_output = self.roberta(input_ids=ids,\n","                                  attention_mask=mask)        \n","            last_layer_hidden_states = roberta_output.last_hidden_state\n","            weights = self.attention(last_layer_hidden_states)\n","            context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)\n","            norm_context_vector = self.layer_norm1(context_vector)\n","            logits = self.linear1(norm_context_vector)\n","            logits = self.linear2(self.layer_norm2(logits)) \n","\n","        return logits"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EeFL7aG-gE71"},"source":["class RobertaLargeAttention(nn.Module):\n","    \n","    def __init__(self, model_type=\"attention\"):\n","        super(RobertaLargeAttention,self).__init__()\n","\n","        self.model_type = model_type\n","        \n","        self.config = AutoConfig.from_pretrained(ROBERTA_LARGE_PATH)\n","        self.config.update({\"output_hidden_states\":True, \n","                       \"hidden_dropout_prob\": 0.0,\n","                       \"layer_norm_eps\": 1e-7})   \n","        self.roberta = transformers.AutoModel.from_pretrained(ROBERTA_LARGE_PATH, config=self.config)\n","\n","        if model_type == \"attention\":\n","            \n","            self.attention = nn.Sequential(            \n","            nn.Linear(1024, 256),            \n","            nn.Tanh(),                       \n","            nn.Linear(256, 1),\n","            nn.Softmax(dim=1)\n","            )   \n","\n","            self.layer_norm1 = nn.LayerNorm(1024)\n","            self.linear1 = nn.Linear(1024, 256)\n","            self.linear2 = nn.Linear(256, 1)\n","            self.layer_norm2 = nn.LayerNorm(256)\n","                           \n","        elif model_type == \"mean\":\n","        \n","            self.layer_norm1 = nn.LayerNorm(1024)\n","            self.linear1 = nn.Linear(1024, 256)\n","            self.linear2 = nn.Linear(256, 1)\n","            self.layer_norm2 = nn.LayerNorm(256)\n","\n","    def freeze(self):\n","        for child in self.roberta.children():\n","            for param in child.parameters():\n","                param.requires_grad = False\n","\n","    def unfreeze(self):\n","        for child in self.roberta.children():\n","            for param in child.parameters():\n","                param.requires_grad = True\n","        \n","    def forward(self, ids, mask, loss_fn = None, targets = None):\n","\n","        if self.model_type == \"mean\":\n","\n","            outputs = self.roberta(ids, mask)\n","            last_hidden_state = outputs[0]\n","            input_mask_expanded = mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n","            sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n","            sum_mask = input_mask_expanded.sum(1)\n","            sum_mask = torch.clamp(sum_mask, min=1e-9)\n","            mean_embeddings = sum_embeddings / sum_mask\n","            norm_mean_embeddings = self.layer_norm1(mean_embeddings)\n","            logits = self.linear1(norm_mean_embeddings)\n","            logits = self.linear2(self.layer_norm2(logits))\n","\n","        elif self.model_type==\"attention\":\n","\n","            roberta_output = self.roberta(input_ids=ids,\n","                                  attention_mask=mask)        \n","            last_layer_hidden_states = roberta_output.last_hidden_state\n","            weights = self.attention(last_layer_hidden_states)\n","            context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)\n","            norm_context_vector = self.layer_norm1(context_vector)\n","            logits = self.linear1(norm_context_vector)\n","            logits = self.linear2(self.layer_norm2(logits)) \n","\n","        return logits"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BUPGPTvugFDD"},"source":["class RobertaLargeMean(nn.Module):\n","    \n","    def __init__(self, model_type=\"mean\"):\n","        super(RobertaLargeMean,self).__init__()\n","\n","        self.model_type = model_type\n","        \n","        self.config = AutoConfig.from_pretrained(ROBERTA_LARGE_PATH)\n","        self.config.update({\"output_hidden_states\":True, \n","                       \"hidden_dropout_prob\": 0.0,\n","                       \"layer_norm_eps\": 1e-7})   \n","        self.roberta = transformers.AutoModel.from_pretrained(ROBERTA_LARGE_PATH, config=self.config)\n","\n","        if model_type == \"attention\":\n","            \n","            self.attention = nn.Sequential(            \n","            nn.Linear(1024, 256),            \n","            nn.Tanh(),                       \n","            nn.Linear(256, 1),\n","            nn.Softmax(dim=1)\n","            )   \n","\n","            self.linear = (nn.Linear(1024, 1))\n","                           \n","        elif model_type == \"mean\":\n","        \n","            self.layer_norm1 = nn.LayerNorm(1024)\n","            self.linear1 = nn.Linear(1024, 768)\n","            self.linear2 = nn.Linear(768, 1)\n","            self.layer_norm2 = nn.LayerNorm(768)\n","\n","    def freeze(self):\n","        for child in self.roberta.children():\n","            for param in child.parameters():\n","                param.requires_grad = False\n","\n","    def unfreeze(self):\n","        for child in self.roberta.children():\n","            for param in child.parameters():\n","                param.requires_grad = True\n","        \n","    def forward(self, ids, mask, loss_fn = None, targets = None):\n","\n","        if self.model_type == \"mean\":\n","\n","            outputs = self.roberta(ids, mask)\n","            last_hidden_state = outputs[0]\n","            input_mask_expanded = mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n","            sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n","            sum_mask = input_mask_expanded.sum(1)\n","            sum_mask = torch.clamp(sum_mask, min=1e-9)\n","            mean_embeddings = sum_embeddings / sum_mask\n","            norm_mean_embeddings = self.layer_norm1(mean_embeddings)\n","            logits = self.linear1(norm_mean_embeddings)\n","            logits = self.linear2(self.layer_norm2(logits))\n","\n","        elif self.model_type==\"attention\":\n","\n","            roberta_output = self.roberta(input_ids=ids,\n","                                  attention_mask=mask)        \n","            last_layer_hidden_states = roberta_output.last_hidden_state\n","            weights = self.attention(last_layer_hidden_states)\n","            context_vector = torch.sum(weights * last_layer_hidden_states, dim=1) \n","            logits = self.linear(context_vector)\n","\n","        return logits"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QLKJnLAEgFGc"},"source":["class ElectraLarge(nn.Module):\n","    \n","    def __init__(self, model_type=\"mean\"):\n","        super(ElectraLarge,self).__init__()\n","\n","        self.model_type = model_type\n","        \n","        self.config = AutoConfig.from_pretrained(ELECTRA_PATH)\n","        self.config.update({ \n","                       \"hidden_dropout_prob\": 0.0,\n","                       \"layer_norm_eps\": 1e-7\n","                       })   \n","        \n","        self.electra = AutoModel.from_pretrained(ELECTRA_PATH, config=self.config)\n","\n","        if model_type == \"attention\":\n","            \n","            self.attention = nn.Sequential(            \n","            nn.Linear(1024, 256),            \n","            nn.Tanh(),                       \n","            nn.Linear(256, 1),\n","            nn.Softmax(dim=1)\n","            )   \n","\n","            self.linear = (nn.Linear(1024, 1))\n","                           \n","        elif model_type == \"mean\":\n","        \n","            self.layer_norm1 = nn.LayerNorm(1024)\n","            self.linear1 = nn.Linear(1024, 768)\n","            self.linear2 = nn.Linear(768, 1)\n","            self.layer_norm2 = nn.LayerNorm(768)\n","        \n","    def forward(self, ids, mask, token_type_ids, loss_fn = None, targets = None):\n","\n","        if self.model_type == \"mean\":\n","\n","            outputs = self.electra(input_ids=ids, attention_mask=mask, token_type_ids = token_type_ids)\n","            last_hidden_state = outputs.last_hidden_state\n","            input_mask_expanded = mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n","            sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n","            sum_mask = input_mask_expanded.sum(1)\n","            sum_mask = torch.clamp(sum_mask, min=1e-9)\n","            mean_embeddings = sum_embeddings / sum_mask\n","            norm_mean_embeddings = self.layer_norm1(mean_embeddings)\n","            logits = self.linear1(norm_mean_embeddings)\n","            logits = self.linear2(self.layer_norm2(logits))\n","\n","        return logits"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_rSYa6gEgRth"},"source":["## Create Predictions"]},{"cell_type":"code","metadata":{"id":"hIglKQh5gQZp"},"source":["def inf_fn(data_loader_electra,data_loader_roberta, model, device):\n","  \"\"\"\n","  Just a simple pytorch function to make predictions.\n","\n","  Args:\n","      data_loader_electra (dataloader) : Training sentences tokenized with electra tokenizer\n","      data_loader_roberta (dataloader) : Training sentences tokenized with roberta tokenizer\n","      model (pytorch model) : the model you want to make predictions with\n","      device : gpu or cpu device\n","  \"\"\"\n","    model.eval()\n","\n","    if \"Electra\" in model.__class__.__name__ :\n","\n","        with torch.no_grad():\n","            for index, d in enumerate(data_loader_electra):\n","                ids = d[\"ids\"]\n","                mask = d[\"mask\"]\n","                token_type_ids = d[\"token_type_ids\"]\n","\n","                ids = ids.to(device, dtype=torch.long)\n","                mask = mask.to(device, dtype=torch.long)\n","                token_type_ids = token_type_ids.to(device, dtype=torch.long)\n","\n","                outputs = model(ids = ids, mask = mask, token_type_ids = token_type_ids)\n","                outputs = outputs.cpu().detach().numpy()\n","\n","                if index == 0:\n","                    preds_test = outputs\n","                else:\n","                    preds_test = np.concatenate((preds_test,outputs), axis=None)\n","\n","    else:\n","\n","        with torch.no_grad():\n","                for index, d in enumerate(data_loader_roberta):\n","                    ids = d[\"ids\"]\n","                    mask = d[\"mask\"]\n","\n","                    ids = ids.to(device, dtype=torch.long)\n","                    mask = mask.to(device, dtype=torch.long)\n","\n","                    outputs = model(ids=ids, mask=mask)\n","\n","                    outputs = outputs.cpu().detach().numpy()\n","\n","                    if index == 0:\n","                        preds_test = outputs\n","                    else:\n","                        preds_test = np.concatenate((preds_test,outputs), axis=None)\n","          \n","    return preds_test"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Yk4wVXLegQg4"},"source":["# create pytorch dataloader\n","def create_dataloader(df):\n","\n","    electra_dataset = ElectraDataset(df)\n","    electra_dataloader = torch.utils.data.DataLoader(electra_dataset, batch_size= 16)\n","    roberta_dataset = RobertaDataset(df)\n","    roberta_dataloader = torch.utils.data.DataLoader(roberta_dataset, batch_size= 16)\n","\n","    return electra_dataloader, roberta_dataloader"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6nlUSr2fgFIj"},"source":["def create_model(device, name, path):\n","    \n","    if (name == \"coderobertalargeattentionnorm2\")|(name== \"RobertaLargeModelsAttention\"):\n","\n","        model = RobertaLargeAttention().to(device)\n","        model.load_state_dict(torch.load(path))\n","\n","    elif (name == \"coderobertalargemean\")|(name == \"RobertaLargeModelsMean\"):\n","\n","        model = RobertaLargeMean().to(device)\n","        model.load_state_dict(torch.load(path))\n","\n","    elif (name == \"codeelectralargebaseline\")|(name == \"ElectraLargeModels\"):\n","\n","        model = ElectraLarge().to(device)\n","        model.load_state_dict(torch.load(path))\n","\n","    elif (name == \"coderobertabaseattentionnorm\")|(name == \"RobertaBaseModels\"):\n","\n","        model = RobertaBaseAttention().to(device)\n","        model.load_state_dict(torch.load(path))\n","\n","    else:\n","        raise Exception(f\"Unknown model: {name}\")\n","    \n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OZghKdvvgcjw"},"source":["def make_predictions(name, path, electra_dataloader, roberta_dataloader ):\n","    device = torch.device(\"cuda\")\n","    model = create_model(device, name, path)\n","    model.load_state_dict(torch.load(path))\n","\n","    results = inf_fn(electra_dataloader,roberta_dataloader , model, device)\n","\n","    return results"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c90NvWWtgjqb"},"source":["## Create Predictions"]},{"cell_type":"code","metadata":{"id":"UTDfi3-O07WX"},"source":["# Weights for each model (based on my best submission in the competition)\n","dict_blend = {'ElectraLargeModels': 0.3, 'RobertaLargeModelsMean': 0.3, 'RobertaLargeModelsAttention': 0.2, 'RobertaBaseModels': 0.2}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rpHF-sQT0JV9"},"source":["def generate_predictions(df, fold_number, dict_blend):\n","  electra_dataloader, roberta_dataloader = create_dataloader(df)\n","  models = [\"ElectraLargeModels\", \"RobertaLargeModelsMean\", \"RobertaLargeModelsAttention\", \"RobertaBaseModels\"]\n","  print(\"Make Predictions\")\n","  predictions = []\n","  for fold in range(5):\n","    for model in models:\n","      print(f\"Inference for fold {fold} for model {model}\")\n","      preds = make_predictions(model, f\"models/{model}/model{fold}.bin\", electra_dataloader, roberta_dataloader)\n","      predictions.append(dict_blend[model] * preds/5)\n","\n","  final_preds = np.sum(predictions, axis=0)\n","\n","  df[\"predictions\"] = final_preds\n","  df.to_csv(f\"pseudo_labels_fold_{fold_number}.csv\", index = False)\n","\n","  return df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Odg5Eg00ntzJ"},"source":["def filter_sentences(df):\n","  \"\"\"\n","  Based on the first solution on Commonlit, a great way to find sentences that are similar to the training set is to compare \n","  the predictions of our best models with the target and standard deviation of its most similar text.\n","\n","  This function enable to keep only the external data where the predictions given are lower targets+stdev and higher than targets-stdev.\n","\n","  Returns:\n","      1 if we need to keep the sentence\n","      0 if not\n","\n","  Args:\n","      df (dataframe) : dataframe with the following columns : sentences, targets, predictions, stdev\n","  \"\"\"\n","  lower_bound = df.targets - df.stdev\n","  higher_bound = df.targets + df.stdev\n","  if (df.predictions>lower_bound) & (df.predictions<higher_bound):\n","    return 1\n","  else:\n","    return 0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KR_Fw-isnw1r"},"source":["def saved_filter_external_data(df,fold):\n","  df[\"to_keep\"] = df.apply(lambda x: filter_sentences(x), axis=1)\n","  df.to_csv(f\"pseudo_labels_fold_{fold}.csv\", index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WecMHgPN1PX-"},"source":["def run_pseudo_labeling():\n","  \n","  for fold in range(5):\n","    path = f\"queries_{fold}.csv\"\n","    target = pd.read_csv(path)\n","    df = generate_predictions(target, fold_number = fold, dict_blend = dict_blend)\n","    saved_filter_external_data(df,fold)\n","    print(f\"Pseudo Labeling Done for fold {fold}\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7H7DMB7Z7OS2"},"source":["run_pseudo_labeling()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pRJ_zU8Hl8BU"},"source":[""],"execution_count":null,"outputs":[]}]}