{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Create_External_Data.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMhtmNsTw6Q12AFRpQBWDWs"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"RBsNJ3c4emDm"},"source":["# Imports"]},{"cell_type":"code","metadata":{"id":"8vTzljgr4q6T"},"source":["!pip install sentence-transformers\n","!pip install datasets"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lQVggH1telPs"},"source":["from datasets import load_dataset\n","from sentence_transformers import SentenceTransformer\n","from pathlib import Path\n","import random\n","import os\n","import torch\n","import pandas as pd\n","import json\n","import gzip"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4qwVLuT7vm_i","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1631781530484,"user_tz":-120,"elapsed":16919,"user":{"displayName":"Gaetan Lopez","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13795704448222064979"}},"outputId":"f5fd49c0-28fa-40c6-c9d3-948d2f66a2c9"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"CTiaRmG8vnGB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1631781530939,"user_tz":-120,"elapsed":464,"user":{"displayName":"Gaetan Lopez","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"13795704448222064979"}},"outputId":"c0f8a939-59bc-429b-b701-ab66bbafa319"},"source":["%cd \"/content/drive/My Drive/CommonLit/External Data\""],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/My Drive/CommonLit/External Data\n"]}]},{"cell_type":"markdown","metadata":{"id":"yWDTY_CfeqGf"},"source":["# Useful functions"]},{"cell_type":"code","metadata":{"id":"_wUYWvlZzQAZ"},"source":["# The code is an attempt to reproduce what the winner of the Competition did with some improvements."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h5fJxwI0ersZ"},"source":["def encode_sentences(sentences, file_name, model_name='paraphrase-TinyBERT-L6-v2' ):\n","\n","  \"\"\"\n","  Creates the embeddings of the sentences using SentenceTransformer.\n","\n","  Takes as input a list of sentences, a file name, and the model used to create the embedding and save the embedding\n","  in a folder called encoded_sentences.\n","\n","  Args:\n","      sentences (list): list of sentences we want to get the embedding from\n","      file_name (str): name of the file\n","      model_name (str): name of the mdoel from hugging face.\n","  \"\"\"\n","\n","  dir_name = Path(\"encoded_sentences\")\n","  # creates a new directory if it does not exist\n","  dir_name.mkdir(parents = True, exist_ok = True)\n","\n","  model = SentenceTransformer(model_name)\n","  # get the embedding of all sentences\n","  encoded_sentences = model.encode(sentences, convert_to_tensor=True)\n","\n","  full_path = os.path.join(\"encoded_sentences\", file_name + '.pt')\n","\n","  # save the file\n","  with open(full_path, 'wb') as f:\n","    torch.save(encoded_sentences, f)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R0-qMn63rsrZ"},"source":["def save_data(data, file_name, dir_name = \"preprocessed_data\"):\n","  \"\"\"\n","  Save the sentences into a folder called dir_name.\n","\n","  Takes as input a list of sentences (data), a file anme and the directory name. \n","\n","  Args:\n","      data (dataframe): dataframe of one column with all the sentences\n","      file_name (str): name of the file\n","      dir_name (str): name of the folder\n","\n","  \"\"\"\n","  dir_name = Path(dir_name)\n","  # creates a new directory if it does not exist\n","  dir_name.mkdir(parents = True, exist_ok = True)\n","  full_path = os.path.join(dir_name, file_name)\n","  # save the dataframe in csv format\n","  data.to_csv(full_path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IaYdqv8oykyG"},"source":["def get_simple_wiki():\n","\n","    \"\"\"\n","    Unzip gzip format and creates a list with all the sentences of simple wiki.\n","\n","    Returns:\n","      List of sentences\n","    \"\"\"\n","    simplewiki_path ='simplewiki-2020-11-01.jsonl.gz'\n","    passages = []\n","    with gzip.open(simplewiki_path, 'rt', encoding='utf8') as fIn:\n","        for line in fIn:\n","            data = json.loads(line.strip())\n","            passages.extend(data['paragraphs'])\n","    return passages"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NSgdaWX4eyum"},"source":["# Preparing Wikipedia Data"]},{"cell_type":"code","metadata":{"id":"GuPo_8NBybze"},"source":["# load the dataset from hugging face\n","wiki_dataset = load_dataset('wikitext', 'wikitext-103-v1', split = \"train\")\n","# filter out sentences with more than 1100 characters and under 700 characters to reproduce the length of the excerpt from the training set\n","wiki_dataset = wiki_dataset.filter(lambda data: len(data[\"text\"])>700 and len(data[\"text\"])<1100)\n","# makes it ready to use in dataframe format\n","wiki_df = wiki_dataset.to_pandas()\n","wiki_sentences = wiki_df.text.tolist()\n","save_data(wiki_df, \"wikipedia.csv\")\n","encode_sentences(wiki_sentences, file_name = \"wikipedia\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"flRDwnIomihH"},"source":["# Preparing Simple Wiki Data"]},{"cell_type":"code","metadata":{"id":"E72CdFaomkk8"},"source":["simplewiki_dataset = get_simple_wiki()\n","simplewiki_filtered = [text for text in simplewiki_dataset if (len(text)<1100 and len(text)>700)]\n","simplewiki_df = pd.DataFrame(simplewiki_filtered)\n","save_data(simplewiki_df, \"simple_wikipedia.csv\")\n","encode_sentences(simplewiki_filtered, file_name = \"simple_wikipedia\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7fKMmeDjvil8"},"source":["# Preparing OneStop English Corpus"]},{"cell_type":"code","metadata":{"id":"fiOeHj2T8DfI"},"source":["# load the dataset from hugging face\n","onestop_data = load_dataset('onestop_english')\n","# only use the training set. Could have used both\n","onestop_data = onestop_data['train']\n","# makes it ready to use in dataframe format\n","onestop_df = onestop_data.to_pandas()\n","onestop_filtered = list(onestop_df.text)\n","save_data(onestop_df, \"onestop.csv\")\n","encode_sentences(onestop_filtered, file_name = \"onestop\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GrALx0AEvk8F"},"source":["# Preparing CBT data"]},{"cell_type":"code","metadata":{"id":"7n5MSlB02ILJ"},"source":["# load the dataset from hugging face\n","dataset = load_dataset(\"cbt\", \"CN\")\n","# makes it ready to use in dataframe format\n","df = dataset[\"train\"].to_pandas()\n","df[\"sentences\"] = df.sentences.apply(lambda x: \" \".join(x))\n","df = df.drop_duplicates(subset = \"sentences\")\n","df[\"sentences\"] = df.sentences.apply(lambda x: x[:1100])\n","cbt_dataset = df[[\"sentences\"]]\n","cbt_dataset.columns = [\"text\"]\n","cbt_filtered = [text for text in cbt_dataset.text.values if (len(text)<1200 and len(text)>700)]\n","cbt_dataset = pd.DataFrame(cbt_filtered)\n","cbt_dataset.columns = [\"text\"]\n","save_data(cbt_dataset, \"cbt.csv\")\n","encode_sentences(cbt_filtered, file_name = \"cbt\")"],"execution_count":null,"outputs":[]}]}