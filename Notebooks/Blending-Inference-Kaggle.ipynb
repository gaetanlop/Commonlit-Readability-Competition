{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2ebf7c0",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.018895,
     "end_time": "2021-09-18T14:29:06.105932",
     "exception": false,
     "start_time": "2021-09-18T14:29:06.087037",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d196569",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-18T14:29:06.152432Z",
     "iopub.status.busy": "2021-09-18T14:29:06.151789Z",
     "iopub.status.idle": "2021-09-18T14:29:12.738339Z",
     "shell.execute_reply": "2021-09-18T14:29:12.737254Z",
     "shell.execute_reply.started": "2021-09-18T14:20:42.193127Z"
    },
    "papermill": {
     "duration": 6.61522,
     "end_time": "2021-09-18T14:29:12.738540",
     "exception": false,
     "start_time": "2021-09-18T14:29:06.123320",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer\n",
    "\n",
    "import random\n",
    "import os\n",
    "from more_itertools import flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7360bcf7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-18T14:29:12.777911Z",
     "iopub.status.busy": "2021-09-18T14:29:12.777332Z",
     "iopub.status.idle": "2021-09-18T14:29:12.795247Z",
     "shell.execute_reply": "2021-09-18T14:29:12.794827Z",
     "shell.execute_reply.started": "2021-09-18T14:20:48.572459Z"
    },
    "papermill": {
     "duration": 0.039361,
     "end_time": "2021-09-18T14:29:12.795360",
     "exception": false,
     "start_time": "2021-09-18T14:29:12.755999",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test=pd.read_csv(\"../input/commonlitreadabilityprize/test.csv\")\n",
    "test[\"target\"]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "968a95dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-18T14:29:12.834212Z",
     "iopub.status.busy": "2021-09-18T14:29:12.833700Z",
     "iopub.status.idle": "2021-09-18T14:29:13.129934Z",
     "shell.execute_reply": "2021-09-18T14:29:13.130365Z",
     "shell.execute_reply.started": "2021-09-18T14:20:48.603022Z"
    },
    "papermill": {
     "duration": 0.317916,
     "end_time": "2021-09-18T14:29:13.130552",
     "exception": false,
     "start_time": "2021-09-18T14:29:12.812636",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 256\n",
    "ROBERTA_LARGE_PATH = \"../input/robertalarge\"\n",
    "ROBERTA_BASE_PATH = \"../input/roberta-new\"\n",
    "ELECTRA_PATH = '../input/electralarge'\n",
    "TEST_BATCH_SIZE = 8\n",
    "TOKENIZER_ROBERTA = transformers.AutoTokenizer.from_pretrained(ROBERTA_LARGE_PATH)\n",
    "TOKENIZER_ELECTRA = AutoTokenizer.from_pretrained(ELECTRA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35577dd2",
   "metadata": {
    "papermill": {
     "duration": 0.017398,
     "end_time": "2021-09-18T14:29:13.165758",
     "exception": false,
     "start_time": "2021-09-18T14:29:13.148360",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d88cefe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-18T14:29:13.209241Z",
     "iopub.status.busy": "2021-09-18T14:29:13.208373Z",
     "iopub.status.idle": "2021-09-18T14:29:13.211267Z",
     "shell.execute_reply": "2021-09-18T14:29:13.210757Z",
     "shell.execute_reply.started": "2021-09-18T14:20:48.956494Z"
    },
    "papermill": {
     "duration": 0.028112,
     "end_time": "2021-09-18T14:29:13.211376",
     "exception": false,
     "start_time": "2021-09-18T14:29:13.183264",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RobertaDataset:\n",
    "    def __init__(self,df):\n",
    "        self.excerpt = df.excerpt.values\n",
    "        self.target = df.target.values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.excerpt)\n",
    "    \n",
    "    def __getitem__(self,item):\n",
    "        excerpt = str(self.excerpt[item])\n",
    "        excerpt = \" \".join(excerpt.split())\n",
    "        inputs = TOKENIZER_ROBERTA(excerpt, add_special_tokens=True, max_length=MAX_LEN, padding=True, truncation=True)\n",
    "        \n",
    "        ids = inputs[\"input_ids\"]\n",
    "        mask = inputs[\"attention_mask\"]\n",
    "        \n",
    "        padding_len = MAX_LEN-len(ids)\n",
    "        ids = ids+([0]*padding_len)\n",
    "        mask = mask+([0]*padding_len)\n",
    " \n",
    "        return {\"ids\": torch.tensor(ids, dtype=torch.long),\n",
    "            \"mask\": torch.tensor(mask, dtype=torch.long),\n",
    "            \"targets\": torch.tensor(self.target[item], dtype=torch.float)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79354776",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-18T14:29:13.255280Z",
     "iopub.status.busy": "2021-09-18T14:29:13.254435Z",
     "iopub.status.idle": "2021-09-18T14:29:13.257155Z",
     "shell.execute_reply": "2021-09-18T14:29:13.256757Z",
     "shell.execute_reply.started": "2021-09-18T14:20:48.965842Z"
    },
    "papermill": {
     "duration": 0.027891,
     "end_time": "2021-09-18T14:29:13.257270",
     "exception": false,
     "start_time": "2021-09-18T14:29:13.229379",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ElectraDataset:\n",
    "    def __init__(self,df):\n",
    "        self.excerpt = df.excerpt.values\n",
    "        self.target = df.target.values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.excerpt)\n",
    "    \n",
    "    def __getitem__(self,item):\n",
    "        excerpt = str(self.excerpt[item])\n",
    "        excerpt = \" \".join(excerpt.split())\n",
    "        inputs = TOKENIZER_ELECTRA(excerpt, add_special_tokens=True, max_length=MAX_LEN, padding=True, truncation=True)\n",
    "        \n",
    "        ids = inputs[\"input_ids\"]\n",
    "        mask = inputs[\"attention_mask\"]\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "        \n",
    "        padding_len = MAX_LEN-len(ids)\n",
    "        ids = ids+([0]*padding_len)\n",
    "        mask = mask+([0]*padding_len)\n",
    "        token_type_ids = token_type_ids+([0]*padding_len)\n",
    " \n",
    "        return {\"ids\": torch.tensor(ids, dtype=torch.long),\n",
    "            \"mask\": torch.tensor(mask, dtype=torch.long),\n",
    "            \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            \"targets\": torch.tensor(self.target[item], dtype=torch.float)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df47cac1",
   "metadata": {
    "papermill": {
     "duration": 0.016901,
     "end_time": "2021-09-18T14:29:13.291744",
     "exception": false,
     "start_time": "2021-09-18T14:29:13.274843",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e770275",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-18T14:29:13.334816Z",
     "iopub.status.busy": "2021-09-18T14:29:13.334013Z",
     "iopub.status.idle": "2021-09-18T14:29:13.336491Z",
     "shell.execute_reply": "2021-09-18T14:29:13.336083Z",
     "shell.execute_reply.started": "2021-09-18T14:20:48.977829Z"
    },
    "papermill": {
     "duration": 0.027155,
     "end_time": "2021-09-18T14:29:13.336616",
     "exception": false,
     "start_time": "2021-09-18T14:29:13.309461",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LitModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        config = AutoConfig.from_pretrained(ROBERTA_BASE_PATH)\n",
    "        config.update({\"output_hidden_states\":True, \n",
    "                       \"hidden_dropout_prob\": 0.0,\n",
    "                       \"layer_norm_eps\": 1e-7})                       \n",
    "        \n",
    "        self.roberta = AutoModel.from_pretrained(ROBERTA_BASE_PATH, config=config)  \n",
    "            \n",
    "        self.attention = nn.Sequential(            \n",
    "            nn.Linear(768, 512),            \n",
    "            nn.Tanh(),                       \n",
    "            nn.Linear(512, 1),\n",
    "            nn.Softmax(dim=1)\n",
    "        )        \n",
    "\n",
    "        self.regressor = nn.Sequential(                        \n",
    "            nn.Linear(768, 1)                        \n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, ids, mask, loss_fn = None, targets = None):\n",
    "        roberta_output = self.roberta(input_ids=ids,\n",
    "                                      attention_mask=mask)        \n",
    "\n",
    "        last_layer_hidden_states = roberta_output.hidden_states[-1]\n",
    "\n",
    "        weights = self.attention(last_layer_hidden_states)\n",
    "                \n",
    "        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)        \n",
    "        \n",
    "        return self.regressor(context_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd348635",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-18T14:29:13.389583Z",
     "iopub.status.busy": "2021-09-18T14:29:13.388630Z",
     "iopub.status.idle": "2021-09-18T14:29:13.391304Z",
     "shell.execute_reply": "2021-09-18T14:29:13.390903Z",
     "shell.execute_reply.started": "2021-09-18T14:20:48.990499Z"
    },
    "papermill": {
     "duration": 0.036357,
     "end_time": "2021-09-18T14:29:13.391415",
     "exception": false,
     "start_time": "2021-09-18T14:29:13.355058",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RobertaBaseAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, model_type=\"attention\"):\n",
    "        super(RobertaBaseAttention,self).__init__()\n",
    "\n",
    "        self.model_type = model_type\n",
    "        \n",
    "        self.config = AutoConfig.from_pretrained(ROBERTA_BASE_PATH)\n",
    "        self.config.update({\"output_hidden_states\":True, \n",
    "                       \"hidden_dropout_prob\": 0.0,\n",
    "                       \"layer_norm_eps\": 1e-7})   \n",
    "        self.roberta = transformers.AutoModel.from_pretrained(ROBERTA_BASE_PATH, config=self.config)\n",
    "\n",
    "        if model_type == \"attention\":\n",
    "            \n",
    "            self.attention = nn.Sequential(            \n",
    "            nn.Linear(768, 256),            \n",
    "            nn.Tanh(),                       \n",
    "            nn.Linear(256, 1),\n",
    "            nn.Softmax(dim=1)\n",
    "            )   \n",
    "\n",
    "            self.layer_norm1 = nn.LayerNorm(768)\n",
    "            self.linear1 = nn.Linear(768, 256)\n",
    "            self.linear2 = nn.Linear(256, 1)\n",
    "            self.layer_norm2 = nn.LayerNorm(256)\n",
    "                           \n",
    "        elif model_type == \"mean\":\n",
    "        \n",
    "            self.layer_norm1 = nn.LayerNorm(1024)\n",
    "            self.linear1 = nn.Linear(1024, 256)\n",
    "            self.linear2 = nn.Linear(256, 1)\n",
    "            self.layer_norm2 = nn.LayerNorm(256)\n",
    "\n",
    "    def freeze(self):\n",
    "        for child in self.roberta.children():\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def unfreeze(self):\n",
    "        for child in self.roberta.children():\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = True\n",
    "        \n",
    "    def forward(self, ids, mask, loss_fn = None, targets = None):\n",
    "\n",
    "        if self.model_type == \"mean\":\n",
    "\n",
    "            outputs = self.roberta(ids, mask)\n",
    "            last_hidden_state = outputs[0]\n",
    "            input_mask_expanded = mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "            sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "            sum_mask = input_mask_expanded.sum(1)\n",
    "            sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "            mean_embeddings = sum_embeddings / sum_mask\n",
    "            norm_mean_embeddings = self.layer_norm1(mean_embeddings)\n",
    "            logits = self.linear1(norm_mean_embeddings)\n",
    "            logits = self.linear2(self.layer_norm2(logits))\n",
    "\n",
    "        elif self.model_type==\"attention\":\n",
    "\n",
    "            roberta_output = self.roberta(input_ids=ids,\n",
    "                                  attention_mask=mask)        \n",
    "            last_layer_hidden_states = roberta_output.last_hidden_state\n",
    "            weights = self.attention(last_layer_hidden_states)\n",
    "            context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)\n",
    "            norm_context_vector = self.layer_norm1(context_vector)\n",
    "            logits = self.linear1(norm_context_vector)\n",
    "            logits = self.linear2(self.layer_norm2(logits)) \n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b9af8b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-18T14:29:13.440159Z",
     "iopub.status.busy": "2021-09-18T14:29:13.439385Z",
     "iopub.status.idle": "2021-09-18T14:29:13.442178Z",
     "shell.execute_reply": "2021-09-18T14:29:13.441728Z",
     "shell.execute_reply.started": "2021-09-18T14:20:49.010035Z"
    },
    "papermill": {
     "duration": 0.033446,
     "end_time": "2021-09-18T14:29:13.442293",
     "exception": false,
     "start_time": "2021-09-18T14:29:13.408847",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RobertaLargeAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, model_type=\"attention\"):\n",
    "        super(RobertaLargeAttention,self).__init__()\n",
    "\n",
    "        self.model_type = model_type\n",
    "        \n",
    "        self.config = AutoConfig.from_pretrained(ROBERTA_LARGE_PATH)\n",
    "        self.config.update({\"output_hidden_states\":True, \n",
    "                       \"hidden_dropout_prob\": 0.0,\n",
    "                       \"layer_norm_eps\": 1e-7})   \n",
    "        self.roberta = transformers.AutoModel.from_pretrained(ROBERTA_LARGE_PATH, config=self.config)\n",
    "\n",
    "        if model_type == \"attention\":\n",
    "            \n",
    "            self.attention = nn.Sequential(            \n",
    "            nn.Linear(1024, 256),            \n",
    "            nn.Tanh(),                       \n",
    "            nn.Linear(256, 1),\n",
    "            nn.Softmax(dim=1)\n",
    "            )   \n",
    "\n",
    "            self.layer_norm1 = nn.LayerNorm(1024)\n",
    "            self.linear1 = nn.Linear(1024, 256)\n",
    "            self.linear2 = nn.Linear(256, 1)\n",
    "            self.layer_norm2 = nn.LayerNorm(256)\n",
    "                           \n",
    "        elif model_type == \"mean\":\n",
    "        \n",
    "            self.layer_norm1 = nn.LayerNorm(1024)\n",
    "            self.linear1 = nn.Linear(1024, 256)\n",
    "            self.linear2 = nn.Linear(256, 1)\n",
    "            self.layer_norm2 = nn.LayerNorm(256)\n",
    "\n",
    "    def freeze(self):\n",
    "        for child in self.roberta.children():\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def unfreeze(self):\n",
    "        for child in self.roberta.children():\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = True\n",
    "        \n",
    "    def forward(self, ids, mask, loss_fn = None, targets = None):\n",
    "\n",
    "        if self.model_type == \"mean\":\n",
    "\n",
    "            outputs = self.roberta(ids, mask)\n",
    "            last_hidden_state = outputs[0]\n",
    "            input_mask_expanded = mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "            sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "            sum_mask = input_mask_expanded.sum(1)\n",
    "            sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "            mean_embeddings = sum_embeddings / sum_mask\n",
    "            norm_mean_embeddings = self.layer_norm1(mean_embeddings)\n",
    "            logits = self.linear1(norm_mean_embeddings)\n",
    "            logits = self.linear2(self.layer_norm2(logits))\n",
    "\n",
    "        elif self.model_type==\"attention\":\n",
    "\n",
    "            roberta_output = self.roberta(input_ids=ids,\n",
    "                                  attention_mask=mask)        \n",
    "            last_layer_hidden_states = roberta_output.last_hidden_state\n",
    "            weights = self.attention(last_layer_hidden_states)\n",
    "            context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)\n",
    "            norm_context_vector = self.layer_norm1(context_vector)\n",
    "            logits = self.linear1(norm_context_vector)\n",
    "            logits = self.linear2(self.layer_norm2(logits)) \n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5eaabf79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-18T14:29:13.489559Z",
     "iopub.status.busy": "2021-09-18T14:29:13.488759Z",
     "iopub.status.idle": "2021-09-18T14:29:13.491238Z",
     "shell.execute_reply": "2021-09-18T14:29:13.490830Z",
     "shell.execute_reply.started": "2021-09-18T14:20:49.028011Z"
    },
    "papermill": {
     "duration": 0.032038,
     "end_time": "2021-09-18T14:29:13.491345",
     "exception": false,
     "start_time": "2021-09-18T14:29:13.459307",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# path : Models/CodeRobertaLargeMean/model{fold}.bin\n",
    "class RobertaLargeMean(nn.Module):\n",
    "    \n",
    "    def __init__(self, model_type=\"mean\"):\n",
    "        super(RobertaLargeMean,self).__init__()\n",
    "\n",
    "        self.model_type = model_type\n",
    "        \n",
    "        self.config = AutoConfig.from_pretrained(ROBERTA_LARGE_PATH)\n",
    "        self.config.update({\"output_hidden_states\":True, \n",
    "                       \"hidden_dropout_prob\": 0.0,\n",
    "                       \"layer_norm_eps\": 1e-7})   \n",
    "        self.roberta = transformers.AutoModel.from_pretrained(ROBERTA_LARGE_PATH, config=self.config)\n",
    "\n",
    "        if model_type == \"attention\":\n",
    "            \n",
    "            self.attention = nn.Sequential(            \n",
    "            nn.Linear(1024, 256),            \n",
    "            nn.Tanh(),                       \n",
    "            nn.Linear(256, 1),\n",
    "            nn.Softmax(dim=1)\n",
    "            )   \n",
    "\n",
    "            self.linear = (nn.Linear(1024, 1))\n",
    "                           \n",
    "        elif model_type == \"mean\":\n",
    "        \n",
    "            self.layer_norm1 = nn.LayerNorm(1024)\n",
    "            self.linear1 = nn.Linear(1024, 768)\n",
    "            self.linear2 = nn.Linear(768, 1)\n",
    "            self.layer_norm2 = nn.LayerNorm(768)\n",
    "\n",
    "    def freeze(self):\n",
    "        for child in self.roberta.children():\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def unfreeze(self):\n",
    "        for child in self.roberta.children():\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = True\n",
    "        \n",
    "    def forward(self, ids, mask, loss_fn = None, targets = None):\n",
    "\n",
    "        if self.model_type == \"mean\":\n",
    "\n",
    "            outputs = self.roberta(ids, mask)\n",
    "            last_hidden_state = outputs[0]\n",
    "            input_mask_expanded = mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "            sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "            sum_mask = input_mask_expanded.sum(1)\n",
    "            sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "            mean_embeddings = sum_embeddings / sum_mask\n",
    "            norm_mean_embeddings = self.layer_norm1(mean_embeddings)\n",
    "            logits = self.linear1(norm_mean_embeddings)\n",
    "            logits = self.linear2(self.layer_norm2(logits))\n",
    "\n",
    "        elif self.model_type==\"attention\":\n",
    "\n",
    "            roberta_output = self.roberta(input_ids=ids,\n",
    "                                  attention_mask=mask)        \n",
    "            last_layer_hidden_states = roberta_output.last_hidden_state\n",
    "            weights = self.attention(last_layer_hidden_states)\n",
    "            context_vector = torch.sum(weights * last_layer_hidden_states, dim=1) \n",
    "            logits = self.linear(context_vector)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04bd2b27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-18T14:29:13.535684Z",
     "iopub.status.busy": "2021-09-18T14:29:13.534971Z",
     "iopub.status.idle": "2021-09-18T14:29:13.537346Z",
     "shell.execute_reply": "2021-09-18T14:29:13.537739Z",
     "shell.execute_reply.started": "2021-09-18T14:20:49.044856Z"
    },
    "papermill": {
     "duration": 0.029424,
     "end_time": "2021-09-18T14:29:13.537865",
     "exception": false,
     "start_time": "2021-09-18T14:29:13.508441",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Attention for roberta path : Models/CodeElectraLargeBaseline/model{fold}.bin\n",
    "class ElectraLarge(nn.Module):\n",
    "    \n",
    "    def __init__(self, model_type=\"mean\"):\n",
    "        super(ElectraLarge,self).__init__()\n",
    "\n",
    "        self.model_type = model_type\n",
    "        \n",
    "        self.config = AutoConfig.from_pretrained(ELECTRA_PATH)\n",
    "        self.config.update({ \n",
    "                       \"hidden_dropout_prob\": 0.0,\n",
    "                       \"layer_norm_eps\": 1e-7\n",
    "                       })   \n",
    "        \n",
    "        self.electra = AutoModel.from_pretrained(ELECTRA_PATH, config=self.config)\n",
    "\n",
    "        if model_type == \"attention\":\n",
    "            \n",
    "            self.attention = nn.Sequential(            \n",
    "            nn.Linear(1024, 256),            \n",
    "            nn.Tanh(),                       \n",
    "            nn.Linear(256, 1),\n",
    "            nn.Softmax(dim=1)\n",
    "            )   \n",
    "\n",
    "            self.linear = (nn.Linear(1024, 1))\n",
    "                           \n",
    "        elif model_type == \"mean\":\n",
    "        \n",
    "            self.layer_norm1 = nn.LayerNorm(1024)\n",
    "            self.linear1 = nn.Linear(1024, 768)\n",
    "            self.linear2 = nn.Linear(768, 1)\n",
    "            self.layer_norm2 = nn.LayerNorm(768)\n",
    "        \n",
    "    def forward(self, ids, mask, token_type_ids, loss_fn = None, targets = None):\n",
    "\n",
    "        if self.model_type == \"mean\":\n",
    "\n",
    "            outputs = self.electra(input_ids=ids, attention_mask=mask, token_type_ids = token_type_ids)\n",
    "            last_hidden_state = outputs.last_hidden_state\n",
    "            input_mask_expanded = mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
    "            sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
    "            sum_mask = input_mask_expanded.sum(1)\n",
    "            sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
    "            mean_embeddings = sum_embeddings / sum_mask\n",
    "            norm_mean_embeddings = self.layer_norm1(mean_embeddings)\n",
    "            logits = self.linear1(norm_mean_embeddings)\n",
    "            logits = self.linear2(self.layer_norm2(logits))\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05959e5b",
   "metadata": {
    "papermill": {
     "duration": 0.01779,
     "end_time": "2021-09-18T14:29:13.572659",
     "exception": false,
     "start_time": "2021-09-18T14:29:13.554869",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Create Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b345ca63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-18T14:29:13.616627Z",
     "iopub.status.busy": "2021-09-18T14:29:13.615791Z",
     "iopub.status.idle": "2021-09-18T14:29:13.618275Z",
     "shell.execute_reply": "2021-09-18T14:29:13.617879Z",
     "shell.execute_reply.started": "2021-09-18T14:20:49.059319Z"
    },
    "papermill": {
     "duration": 0.028731,
     "end_time": "2021-09-18T14:29:13.618379",
     "exception": false,
     "start_time": "2021-09-18T14:29:13.589648",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def inf_fn(data_loader_electra,data_loader_roberta, model, device):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    if \"Electra\" in model.__class__.__name__ :\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for index, d in enumerate(data_loader_electra):\n",
    "                ids = d[\"ids\"]\n",
    "                mask = d[\"mask\"]\n",
    "                token_type_ids = d[\"token_type_ids\"]\n",
    "\n",
    "                ids = ids.to(device, dtype=torch.long)\n",
    "                mask = mask.to(device, dtype=torch.long)\n",
    "                token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
    "\n",
    "                outputs = model(ids = ids, mask = mask, token_type_ids = token_type_ids)\n",
    "                outputs = outputs.cpu().detach().numpy()\n",
    "\n",
    "                if index == 0:\n",
    "                    preds_test = outputs\n",
    "                else:\n",
    "                    preds_test = np.concatenate((preds_test,outputs), axis=None)\n",
    "\n",
    "    else:\n",
    "\n",
    "        with torch.no_grad():\n",
    "                for index, d in enumerate(data_loader_roberta):\n",
    "                    ids = d[\"ids\"]\n",
    "                    mask = d[\"mask\"]\n",
    "\n",
    "                    ids = ids.to(device, dtype=torch.long)\n",
    "                    mask = mask.to(device, dtype=torch.long)\n",
    "\n",
    "                    outputs = model(ids=ids, mask=mask)\n",
    "\n",
    "                    outputs = outputs.cpu().detach().numpy()\n",
    "\n",
    "                    if index == 0:\n",
    "                        preds_test = outputs\n",
    "                    else:\n",
    "                        preds_test = np.concatenate((preds_test,outputs), axis=None)\n",
    "          \n",
    "    return preds_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c355f00",
   "metadata": {
    "papermill": {
     "duration": 0.016876,
     "end_time": "2021-09-18T14:29:13.652175",
     "exception": false,
     "start_time": "2021-09-18T14:29:13.635299",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Useful Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1921321f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-18T14:29:13.691651Z",
     "iopub.status.busy": "2021-09-18T14:29:13.690848Z",
     "iopub.status.idle": "2021-09-18T14:29:13.693396Z",
     "shell.execute_reply": "2021-09-18T14:29:13.693000Z",
     "shell.execute_reply.started": "2021-09-18T14:20:49.073139Z"
    },
    "papermill": {
     "duration": 0.024127,
     "end_time": "2021-09-18T14:29:13.693501",
     "exception": false,
     "start_time": "2021-09-18T14:29:13.669374",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create pytorch dataloader\n",
    "def create_dataloader():\n",
    "\n",
    "    valid_electra_dataset = ElectraDataset(test)\n",
    "    valid_electra_dataloader = torch.utils.data.DataLoader(valid_electra_dataset, batch_size= TEST_BATCH_SIZE)\n",
    "    valid_roberta_dataset = RobertaDataset(test)\n",
    "    valid_roberta_dataloader = torch.utils.data.DataLoader(valid_roberta_dataset, batch_size= TEST_BATCH_SIZE)\n",
    "\n",
    "    return valid_electra_dataloader, valid_roberta_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d95437b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-18T14:29:13.734264Z",
     "iopub.status.busy": "2021-09-18T14:29:13.733486Z",
     "iopub.status.idle": "2021-09-18T14:29:13.735724Z",
     "shell.execute_reply": "2021-09-18T14:29:13.736152Z",
     "shell.execute_reply.started": "2021-09-18T14:26:53.219972Z"
    },
    "papermill": {
     "duration": 0.025814,
     "end_time": "2021-09-18T14:29:13.736280",
     "exception": false,
     "start_time": "2021-09-18T14:29:13.710466",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_model(device, name, path):\n",
    "    \n",
    "    if name == \"coderobertalargeattentionnorm2\":\n",
    "\n",
    "        model = RobertaLargeAttention().to(device)\n",
    "        model.load_state_dict(torch.load(path))\n",
    "\n",
    "    elif name == \"coderobertalargemean\":\n",
    "\n",
    "        model = RobertaLargeMean().to(device)\n",
    "        model.load_state_dict(torch.load(path))\n",
    "\n",
    "    elif name == \"electra\":\n",
    "\n",
    "        model = ElectraLarge().to(device)\n",
    "        model.load_state_dict(torch.load(path))\n",
    "\n",
    "    elif name == \"coderobertabaseattentionnorm\":\n",
    "\n",
    "        model = RobertaBaseAttention().to(device)\n",
    "        model.load_state_dict(torch.load(path))\n",
    "    \n",
    "\n",
    "    else:\n",
    "        raise Exception(f\"Unknown model: {name}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1aa25772",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-18T14:29:13.775044Z",
     "iopub.status.busy": "2021-09-18T14:29:13.774316Z",
     "iopub.status.idle": "2021-09-18T14:29:13.776636Z",
     "shell.execute_reply": "2021-09-18T14:29:13.777042Z",
     "shell.execute_reply.started": "2021-09-18T14:26:53.386760Z"
    },
    "papermill": {
     "duration": 0.023861,
     "end_time": "2021-09-18T14:29:13.777161",
     "exception": false,
     "start_time": "2021-09-18T14:29:13.753300",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_predictions(test, name, path, valid_electra_dataloader, valid_roberta_dataloader ):\n",
    "    device = torch.device(\"cuda\")\n",
    "    model = create_model(device, name, path)\n",
    "    model.load_state_dict(torch.load(path))\n",
    "\n",
    "    results = inf_fn(valid_electra_dataloader,valid_roberta_dataloader , model, device)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a05f629",
   "metadata": {
    "papermill": {
     "duration": 0.016774,
     "end_time": "2021-09-18T14:29:13.810876",
     "exception": false,
     "start_time": "2021-09-18T14:29:13.794102",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Create Submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ed9081f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-18T14:29:13.849130Z",
     "iopub.status.busy": "2021-09-18T14:29:13.848383Z",
     "iopub.status.idle": "2021-09-18T14:29:13.851111Z",
     "shell.execute_reply": "2021-09-18T14:29:13.850620Z",
     "shell.execute_reply.started": "2021-09-18T14:26:53.669872Z"
    },
    "papermill": {
     "duration": 0.023369,
     "end_time": "2021-09-18T14:29:13.851212",
     "exception": false,
     "start_time": "2021-09-18T14:29:13.827843",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "valid_electra_dataloader, valid_roberta_dataloader = create_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e6f75463",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-18T14:29:13.890072Z",
     "iopub.status.busy": "2021-09-18T14:29:13.889308Z",
     "iopub.status.idle": "2021-09-18T14:29:13.892024Z",
     "shell.execute_reply": "2021-09-18T14:29:13.891488Z",
     "shell.execute_reply.started": "2021-09-18T14:26:53.789498Z"
    },
    "papermill": {
     "duration": 0.023385,
     "end_time": "2021-09-18T14:29:13.892129",
     "exception": false,
     "start_time": "2021-09-18T14:29:13.868744",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dict_blend = {'electra': 0.3, 'coderobertalargemean': 0.3, 'coderobertalargeattentionnorm2': 0.2, 'coderobertabaseattentionnorm': 0.2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d0adf50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-18T14:29:13.932794Z",
     "iopub.status.busy": "2021-09-18T14:29:13.932143Z",
     "iopub.status.idle": "2021-09-18T14:36:22.707369Z",
     "shell.execute_reply": "2021-09-18T14:36:22.706784Z"
    },
    "papermill": {
     "duration": 428.799812,
     "end_time": "2021-09-18T14:36:22.708989",
     "exception": false,
     "start_time": "2021-09-18T14:29:13.909177",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference for fold 0 for model coderobertabaseattentionnorm\n",
      "Inference for fold 0 for model electra\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../input/electralarge were not used when initializing ElectraModel: ['discriminator_predictions.dense.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense_prediction.weight']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference for fold 0 for model coderobertalargemean\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../input/robertalarge were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference for fold 0 for model coderobertalargeattentionnorm2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../input/robertalarge were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference for fold 1 for model coderobertabaseattentionnorm\n",
      "Inference for fold 1 for model electra\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../input/electralarge were not used when initializing ElectraModel: ['discriminator_predictions.dense.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense_prediction.weight']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference for fold 1 for model coderobertalargemean\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../input/robertalarge were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference for fold 1 for model coderobertalargeattentionnorm2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../input/robertalarge were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference for fold 2 for model coderobertabaseattentionnorm\n",
      "Inference for fold 2 for model electra\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../input/electralarge were not used when initializing ElectraModel: ['discriminator_predictions.dense.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense_prediction.weight']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference for fold 2 for model coderobertalargemean\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../input/robertalarge were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference for fold 2 for model coderobertalargeattentionnorm2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../input/robertalarge were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference for fold 3 for model coderobertabaseattentionnorm\n",
      "Inference for fold 3 for model electra\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../input/electralarge were not used when initializing ElectraModel: ['discriminator_predictions.dense.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense_prediction.weight']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference for fold 3 for model coderobertalargemean\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../input/robertalarge were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference for fold 3 for model coderobertalargeattentionnorm2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../input/robertalarge were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference for fold 4 for model coderobertabaseattentionnorm\n",
      "Inference for fold 4 for model electra\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../input/electralarge were not used when initializing ElectraModel: ['discriminator_predictions.dense.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense_prediction.weight']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference for fold 4 for model coderobertalargemean\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../input/robertalarge were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference for fold 4 for model coderobertalargeattentionnorm2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../input/robertalarge were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "models = [\"coderobertabaseattentionnorm\", \"electra\", \"coderobertalargemean\", \"coderobertalargeattentionnorm2\"]\n",
    "predictions = []\n",
    "for fold in range(5):\n",
    "    for model in models:\n",
    "        print(f\"Inference for fold {fold} for model {model}\")\n",
    "            \n",
    "        preds = make_predictions(test,model, f\"../input/{model}/model{fold}.bin\", valid_electra_dataloader, valid_roberta_dataloader)\n",
    "        predictions.append(dict_blend[model] * preds/5)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d69aaba2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-18T14:36:22.775494Z",
     "iopub.status.busy": "2021-09-18T14:36:22.774237Z",
     "iopub.status.idle": "2021-09-18T14:36:22.776650Z",
     "shell.execute_reply": "2021-09-18T14:36:22.777067Z"
    },
    "papermill": {
     "duration": 0.041595,
     "end_time": "2021-09-18T14:36:22.777185",
     "exception": false,
     "start_time": "2021-09-18T14:36:22.735590",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dict_blend = {'codeelectralargebaseline': 0.3, 'coderobertalargemean': 0.3, 'coderobertalargeattentionnorm2': 0.2, 'coderobertabaseattentionnorm': 0.2}\n",
    "# models = [\"codeelectralargebaseline\", \"coderobertalargemean\", \"coderobertalargeattentionnorm2\", \"coderobertabaseattentionnorm\"]\n",
    "# predictions = []\n",
    "# for fold in range(5):\n",
    "#     for model in models:\n",
    "#         print(f\"Inference for fold {fold} for model {model}\")\n",
    "#         preds = make_predictions(test,model, f\"../input/{model}/model{fold}.bin\", valid_electra_dataloader, valid_roberta_dataloader)\n",
    "#         predictions.append(dict_blend[model] * preds/5)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fbd94a0c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-18T14:36:22.839171Z",
     "iopub.status.busy": "2021-09-18T14:36:22.838600Z",
     "iopub.status.idle": "2021-09-18T14:36:22.908553Z",
     "shell.execute_reply": "2021-09-18T14:36:22.908952Z"
    },
    "papermill": {
     "duration": 0.105621,
     "end_time": "2021-09-18T14:36:22.909075",
     "exception": false,
     "start_time": "2021-09-18T14:36:22.803454",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>c0f722661</td>\n",
       "      <td>-0.418750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f0953f0a5</td>\n",
       "      <td>-0.354951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0df072751</td>\n",
       "      <td>-0.430480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>04caf4e0c</td>\n",
       "      <td>-2.353032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0e63f8bea</td>\n",
       "      <td>-1.903276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>12537fe78</td>\n",
       "      <td>-1.224088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>965e592c0</td>\n",
       "      <td>0.276594</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id    target\n",
       "0  c0f722661 -0.418750\n",
       "1  f0953f0a5 -0.354951\n",
       "2  0df072751 -0.430480\n",
       "3  04caf4e0c -2.353032\n",
       "4  0e63f8bea -1.903276\n",
       "5  12537fe78 -1.224088\n",
       "6  965e592c0  0.276594"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.sum(predictions, axis=0)\n",
    "\n",
    "test.target=a\n",
    "test=test[[\"id\",\"target\"]]\n",
    "test.to_csv(\"submission.csv\",index=False)\n",
    "test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f968b9ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-18T14:36:22.965426Z",
     "iopub.status.busy": "2021-09-18T14:36:22.964956Z",
     "iopub.status.idle": "2021-09-18T14:36:22.968704Z",
     "shell.execute_reply": "2021-09-18T14:36:22.968272Z",
     "shell.execute_reply.started": "2021-09-08T15:41:58.299034Z"
    },
    "papermill": {
     "duration": 0.033102,
     "end_time": "2021-09-18T14:36:22.968810",
     "exception": false,
     "start_time": "2021-09-18T14:36:22.935708",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 23h30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c737deb4",
   "metadata": {
    "papermill": {
     "duration": 0.026209,
     "end_time": "2021-09-18T14:36:23.021248",
     "exception": false,
     "start_time": "2021-09-18T14:36:22.995039",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 446.677856,
   "end_time": "2021-09-18T14:36:26.328635",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-09-18T14:28:59.650779",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
