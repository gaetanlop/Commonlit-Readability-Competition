{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Code Fold Optuna",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_oLRpv3Yz8M"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ch7iOuPEZW6J",
        "outputId": "5a59d392-3bc7-4c1e-8758-103f33ab56bb"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N810KgfmZFWl",
        "outputId": "24d8dfb2-2c15-4270-b532-721c1482ee46"
      },
      "source": [
        "pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.9.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (5.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: huggingface-hub==0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.12)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7LZMQyiY1I3"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "import os\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm_notebook\n",
        "\n",
        "from transformers import AutoConfig, AutoTokenizer, AutoModel, ElectraConfig, ElectraModel, ElectraTokenizer\n",
        "import transformers\n",
        "from transformers import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
        "\n",
        "import torch.nn.init as init\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Parameter\n",
        "from torch.autograd.function import InplaceFunction\n",
        "import math\n",
        "\n",
        "from torch.utils.data import Sampler, Dataset, DataLoader\n",
        "import random\n",
        "\n",
        "from more_itertools import chunked, flatten"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4k-dmgh_Y4BO",
        "outputId": "885665ef-01fa-4ff6-c111-e0d9fb608e7e"
      },
      "source": [
        "%cd drive/MyDrive/CommonLit"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/CommonLit\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJbmbN0tpgqQ"
      },
      "source": [
        "# Get folds"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akuiMAmCaEaF"
      },
      "source": [
        "df = pd.read_csv(\"train_folds.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wc6U6Of0aiB2"
      },
      "source": [
        "# Seed Everything"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xMz3-kDaTkA"
      },
      "source": [
        "def seed_everything(seed=12):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "seed_everything(seed=12)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPHzEnpEdbq0"
      },
      "source": [
        "# Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TusK1jchaxBH"
      },
      "source": [
        "MAX_LEN = 256\n",
        "EPOCHS = 4\n",
        "ROBERTA_LARGE_PATH = \"roberta-large\"\n",
        "ROBERTA_BASE_PATH = \"roberta-base\"\n",
        "ELECTRA_PATH = 'google/electra-large-discriminator'\n",
        "TEST_BATCH_SIZE = 32\n",
        "TOKENIZER_ROBERTA = transformers.AutoTokenizer.from_pretrained(ROBERTA_LARGE_PATH)\n",
        "TOKENIZER_ELECTRA = ElectraTokenizer.from_pretrained(ELECTRA_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HMMTxqWff7x"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rklfe2j9dewv"
      },
      "source": [
        "class RobertaDataset:\n",
        "    def __init__(self,df):\n",
        "        self.excerpt = df.excerpt.values\n",
        "        self.target = df.target.values\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.excerpt)\n",
        "    \n",
        "    def __getitem__(self,item):\n",
        "        excerpt = str(self.excerpt[item])\n",
        "        excerpt = \" \".join(excerpt.split())\n",
        "        inputs = TOKENIZER_ROBERTA(excerpt, add_special_tokens=True, max_length=MAX_LEN, padding=True, truncation=True)\n",
        "        \n",
        "        ids = inputs[\"input_ids\"]\n",
        "        mask = inputs[\"attention_mask\"]\n",
        "        \n",
        "        padding_len = MAX_LEN-len(ids)\n",
        "        ids = ids+([0]*padding_len)\n",
        "        mask = mask+([0]*padding_len)\n",
        " \n",
        "        return {\"ids\": torch.tensor(ids, dtype=torch.long),\n",
        "            \"mask\": torch.tensor(mask, dtype=torch.long),\n",
        "            \"targets\": torch.tensor(self.target[item], dtype=torch.float)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H903R6Vw36Ox"
      },
      "source": [
        "class ElectraDataset:\n",
        "    def __init__(self,df):\n",
        "        self.excerpt = df.excerpt.values\n",
        "        self.target = df.target.values\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.excerpt)\n",
        "    \n",
        "    def __getitem__(self,item):\n",
        "        excerpt = str(self.excerpt[item])\n",
        "        excerpt = \" \".join(excerpt.split())\n",
        "        inputs = TOKENIZER_ELECTRA(excerpt, add_special_tokens=True, max_length=MAX_LEN, padding=True, truncation=True)\n",
        "        \n",
        "        ids = inputs[\"input_ids\"]\n",
        "        mask = inputs[\"attention_mask\"]\n",
        "        token_type_ids = inputs[\"token_type_ids\"]\n",
        "        \n",
        "        padding_len = MAX_LEN-len(ids)\n",
        "        ids = ids+([0]*padding_len)\n",
        "        mask = mask+([0]*padding_len)\n",
        "        token_type_ids = token_type_ids+([0]*padding_len)\n",
        " \n",
        "        return {\"ids\": torch.tensor(ids, dtype=torch.long),\n",
        "            \"mask\": torch.tensor(mask, dtype=torch.long),\n",
        "            \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n",
        "            \"targets\": torch.tensor(self.target[item], dtype=torch.float)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Urdqz9g8fhwW"
      },
      "source": [
        "# Model Building"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSz02kb95M-F"
      },
      "source": [
        "# path Models/RobertaBaseAttentionNorm/model{fold}.bin\n",
        "class RobertaBaseAttention(nn.Module):\n",
        "    \n",
        "    def __init__(self, model_type=\"attention\"):\n",
        "        super(RobertaBaseAttention,self).__init__()\n",
        "\n",
        "        self.model_type = model_type\n",
        "        \n",
        "        self.config = AutoConfig.from_pretrained(ROBERTA_BASE_PATH)\n",
        "        self.config.update({\"output_hidden_states\":True, \n",
        "                       \"hidden_dropout_prob\": 0.0,\n",
        "                       \"layer_norm_eps\": 1e-7})   \n",
        "        self.roberta = transformers.AutoModel.from_pretrained(ROBERTA_BASE_PATH, config=self.config)\n",
        "\n",
        "        if model_type == \"attention\":\n",
        "            \n",
        "            self.attention = nn.Sequential(            \n",
        "            nn.Linear(768, 256),            \n",
        "            nn.Tanh(),                       \n",
        "            nn.Linear(256, 1),\n",
        "            nn.Softmax(dim=1)\n",
        "            )   \n",
        "\n",
        "            self.layer_norm1 = nn.LayerNorm(768)\n",
        "            self.linear1 = nn.Linear(768, 256)\n",
        "            self.linear2 = nn.Linear(256, 1)\n",
        "            self.layer_norm2 = nn.LayerNorm(256)\n",
        "                           \n",
        "        elif model_type == \"mean\":\n",
        "        \n",
        "            self.layer_norm1 = nn.LayerNorm(1024)\n",
        "            self.linear1 = nn.Linear(1024, 256)\n",
        "            self.linear2 = nn.Linear(256, 1)\n",
        "            self.layer_norm2 = nn.LayerNorm(256)\n",
        "\n",
        "    def freeze(self):\n",
        "        for child in self.roberta.children():\n",
        "            for param in child.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def unfreeze(self):\n",
        "        for child in self.roberta.children():\n",
        "            for param in child.parameters():\n",
        "                param.requires_grad = True\n",
        "        \n",
        "    def forward(self, ids, mask, loss_fn = None, targets = None):\n",
        "\n",
        "        if self.model_type == \"mean\":\n",
        "\n",
        "            outputs = self.roberta(ids, mask)\n",
        "            last_hidden_state = outputs[0]\n",
        "            input_mask_expanded = mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
        "            sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
        "            sum_mask = input_mask_expanded.sum(1)\n",
        "            sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
        "            mean_embeddings = sum_embeddings / sum_mask\n",
        "            norm_mean_embeddings = self.layer_norm1(mean_embeddings)\n",
        "            logits = self.linear1(norm_mean_embeddings)\n",
        "            logits = self.linear2(self.layer_norm2(logits))\n",
        "\n",
        "        elif self.model_type==\"attention\":\n",
        "\n",
        "            roberta_output = self.roberta(input_ids=ids,\n",
        "                                  attention_mask=mask)        \n",
        "            last_layer_hidden_states = roberta_output.last_hidden_state\n",
        "            weights = self.attention(last_layer_hidden_states)\n",
        "            context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)\n",
        "            norm_context_vector = self.layer_norm1(context_vector)\n",
        "            logits = self.linear1(norm_context_vector)\n",
        "            logits = self.linear2(self.layer_norm2(logits)) \n",
        "\n",
        "        return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDW8Do7K4uZ6"
      },
      "source": [
        "# path : CodeRobertaLargeAttentionNorm2\n",
        "class RobertaLargeAttention(nn.Module):\n",
        "    \n",
        "    def __init__(self, model_type=\"attention\"):\n",
        "        super(RobertaLargeAttention,self).__init__()\n",
        "\n",
        "        self.model_type = model_type\n",
        "        \n",
        "        self.config = AutoConfig.from_pretrained(ROBERTA_LARGE_PATH)\n",
        "        self.config.update({\"output_hidden_states\":True, \n",
        "                       \"hidden_dropout_prob\": 0.0,\n",
        "                       \"layer_norm_eps\": 1e-7})   \n",
        "        self.roberta = transformers.AutoModel.from_pretrained(ROBERTA_LARGE_PATH, config=self.config)\n",
        "\n",
        "        if model_type == \"attention\":\n",
        "            \n",
        "            self.attention = nn.Sequential(            \n",
        "            nn.Linear(1024, 256),            \n",
        "            nn.Tanh(),                       \n",
        "            nn.Linear(256, 1),\n",
        "            nn.Softmax(dim=1)\n",
        "            )   \n",
        "\n",
        "            self.layer_norm1 = nn.LayerNorm(1024)\n",
        "            self.linear1 = nn.Linear(1024, 256)\n",
        "            self.linear2 = nn.Linear(256, 1)\n",
        "            self.layer_norm2 = nn.LayerNorm(256)\n",
        "                           \n",
        "        elif model_type == \"mean\":\n",
        "        \n",
        "            self.layer_norm1 = nn.LayerNorm(1024)\n",
        "            self.linear1 = nn.Linear(1024, 256)\n",
        "            self.linear2 = nn.Linear(256, 1)\n",
        "            self.layer_norm2 = nn.LayerNorm(256)\n",
        "\n",
        "    def freeze(self):\n",
        "        for child in self.roberta.children():\n",
        "            for param in child.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def unfreeze(self):\n",
        "        for child in self.roberta.children():\n",
        "            for param in child.parameters():\n",
        "                param.requires_grad = True\n",
        "        \n",
        "    def forward(self, ids, mask, loss_fn = None, targets = None):\n",
        "\n",
        "        if self.model_type == \"mean\":\n",
        "\n",
        "            outputs = self.roberta(ids, mask)\n",
        "            last_hidden_state = outputs[0]\n",
        "            input_mask_expanded = mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
        "            sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
        "            sum_mask = input_mask_expanded.sum(1)\n",
        "            sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
        "            mean_embeddings = sum_embeddings / sum_mask\n",
        "            norm_mean_embeddings = self.layer_norm1(mean_embeddings)\n",
        "            logits = self.linear1(norm_mean_embeddings)\n",
        "            logits = self.linear2(self.layer_norm2(logits))\n",
        "\n",
        "        elif self.model_type==\"attention\":\n",
        "\n",
        "            roberta_output = self.roberta(input_ids=ids,\n",
        "                                  attention_mask=mask)        \n",
        "            last_layer_hidden_states = roberta_output.last_hidden_state\n",
        "            weights = self.attention(last_layer_hidden_states)\n",
        "            context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)\n",
        "            norm_context_vector = self.layer_norm1(context_vector)\n",
        "            logits = self.linear1(norm_context_vector)\n",
        "            logits = self.linear2(self.layer_norm2(logits)) \n",
        "\n",
        "        return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLXbHlx_fi0b"
      },
      "source": [
        "# path : Models/CodeRobertaLargeMean/model{fold}.bin\n",
        "class RobertaLargeMean(nn.Module):\n",
        "    \n",
        "    def __init__(self, model_type=\"mean\"):\n",
        "        super(RobertaLargeMean,self).__init__()\n",
        "\n",
        "        self.model_type = model_type\n",
        "        \n",
        "        self.config = AutoConfig.from_pretrained(ROBERTA_LARGE_PATH)\n",
        "        self.config.update({\"output_hidden_states\":True, \n",
        "                       \"hidden_dropout_prob\": 0.0,\n",
        "                       \"layer_norm_eps\": 1e-7})   \n",
        "        self.roberta = transformers.AutoModel.from_pretrained(ROBERTA_LARGE_PATH, config=self.config)\n",
        "\n",
        "        if model_type == \"attention\":\n",
        "            \n",
        "            self.attention = nn.Sequential(            \n",
        "            nn.Linear(1024, 256),            \n",
        "            nn.Tanh(),                       \n",
        "            nn.Linear(256, 1),\n",
        "            nn.Softmax(dim=1)\n",
        "            )   \n",
        "\n",
        "            self.linear = (nn.Linear(1024, 1))\n",
        "                           \n",
        "        elif model_type == \"mean\":\n",
        "        \n",
        "            self.layer_norm1 = nn.LayerNorm(1024)\n",
        "            self.linear1 = nn.Linear(1024, 768)\n",
        "            self.linear2 = nn.Linear(768, 1)\n",
        "            self.layer_norm2 = nn.LayerNorm(768)\n",
        "\n",
        "    def freeze(self):\n",
        "        for child in self.roberta.children():\n",
        "            for param in child.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def unfreeze(self):\n",
        "        for child in self.roberta.children():\n",
        "            for param in child.parameters():\n",
        "                param.requires_grad = True\n",
        "        \n",
        "    def forward(self, ids, mask, loss_fn = None, targets = None):\n",
        "\n",
        "        if self.model_type == \"mean\":\n",
        "\n",
        "            outputs = self.roberta(ids, mask)\n",
        "            last_hidden_state = outputs[0]\n",
        "            input_mask_expanded = mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
        "            sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
        "            sum_mask = input_mask_expanded.sum(1)\n",
        "            sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
        "            mean_embeddings = sum_embeddings / sum_mask\n",
        "            norm_mean_embeddings = self.layer_norm1(mean_embeddings)\n",
        "            logits = self.linear1(norm_mean_embeddings)\n",
        "            logits = self.linear2(self.layer_norm2(logits))\n",
        "\n",
        "        elif self.model_type==\"attention\":\n",
        "\n",
        "            roberta_output = self.roberta(input_ids=ids,\n",
        "                                  attention_mask=mask)        \n",
        "            last_layer_hidden_states = roberta_output.last_hidden_state\n",
        "            weights = self.attention(last_layer_hidden_states)\n",
        "            context_vector = torch.sum(weights * last_layer_hidden_states, dim=1) \n",
        "            logits = self.linear(context_vector)\n",
        "\n",
        "        return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fi1IP1TK383A"
      },
      "source": [
        "# Attention for roberta path : Models/CodeElectraLargeBaseline/model{fold}.bin\n",
        "class ElectraLarge(nn.Module):\n",
        "    \n",
        "    def __init__(self, model_type=\"mean\"):\n",
        "        super(ElectraLarge,self).__init__()\n",
        "\n",
        "        self.model_type = model_type\n",
        "        \n",
        "        self.config = ElectraConfig.from_pretrained(ELECTRA_PATH)\n",
        "        self.config.update({ \n",
        "                       \"hidden_dropout_prob\": 0.0,\n",
        "                       \"layer_norm_eps\": 1e-7\n",
        "                       })   \n",
        "        \n",
        "        self.electra = ElectraModel.from_pretrained(ELECTRA_PATH, config=self.config)\n",
        "\n",
        "        if model_type == \"attention\":\n",
        "            \n",
        "            self.attention = nn.Sequential(            \n",
        "            nn.Linear(1024, 256),            \n",
        "            nn.Tanh(),                       \n",
        "            nn.Linear(256, 1),\n",
        "            nn.Softmax(dim=1)\n",
        "            )   \n",
        "\n",
        "            self.linear = (nn.Linear(1024, 1))\n",
        "                           \n",
        "        elif model_type == \"mean\":\n",
        "        \n",
        "            self.layer_norm1 = nn.LayerNorm(1024)\n",
        "            self.linear1 = nn.Linear(1024, 768)\n",
        "            self.linear2 = nn.Linear(768, 1)\n",
        "            self.layer_norm2 = nn.LayerNorm(768)\n",
        "        \n",
        "    def forward(self, ids, mask, token_type_ids, loss_fn = None, targets = None):\n",
        "\n",
        "        if self.model_type == \"mean\":\n",
        "\n",
        "            outputs = self.electra(input_ids=ids, attention_mask=mask, token_type_ids = token_type_ids)\n",
        "            last_hidden_state = outputs.last_hidden_state\n",
        "            input_mask_expanded = mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n",
        "            sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n",
        "            sum_mask = input_mask_expanded.sum(1)\n",
        "            sum_mask = torch.clamp(sum_mask, min=1e-9)\n",
        "            mean_embeddings = sum_embeddings / sum_mask\n",
        "            norm_mean_embeddings = self.layer_norm1(mean_embeddings)\n",
        "            logits = self.linear1(norm_mean_embeddings)\n",
        "            logits = self.linear2(self.layer_norm2(logits))\n",
        "\n",
        "        return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vQK4ByU7Vkj"
      },
      "source": [
        "# Create Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EIlTaOSN7VLM"
      },
      "source": [
        "def inf_fn(data_loader_electra,data_loader_roberta, model, device):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    if \"Electra\" in model.__class__.__name__ :\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for index, d in enumerate(data_loader_electra):\n",
        "                ids = d[\"ids\"]\n",
        "                mask = d[\"mask\"]\n",
        "                token_type_ids = d[\"token_type_ids\"]\n",
        "\n",
        "                ids = ids.to(device, dtype=torch.long)\n",
        "                mask = mask.to(device, dtype=torch.long)\n",
        "                token_type_ids = token_type_ids.to(device, dtype=torch.long)\n",
        "\n",
        "                outputs = model(ids = ids, mask = mask, token_type_ids = token_type_ids)\n",
        "                outputs = outputs.cpu().detach().numpy()\n",
        "\n",
        "                if index == 0:\n",
        "                    preds_test = outputs\n",
        "                else:\n",
        "                    preds_test = np.concatenate((preds_test,outputs), axis=None)\n",
        "\n",
        "    else:\n",
        "\n",
        "        with torch.no_grad():\n",
        "          for index, d in enumerate(data_loader_roberta):\n",
        "              ids = d[\"ids\"]\n",
        "              mask = d[\"mask\"]\n",
        "\n",
        "              ids = ids.to(device, dtype=torch.long)\n",
        "              mask = mask.to(device, dtype=torch.long)\n",
        "\n",
        "              outputs = model(ids=ids, mask=mask)\n",
        "\n",
        "              outputs = outputs.cpu().detach().numpy()\n",
        "\n",
        "              if index == 0:\n",
        "                  preds_test = outputs\n",
        "              else:\n",
        "                  preds_test = np.concatenate((preds_test,outputs), axis=None)\n",
        "          \n",
        "    return preds_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2zQuwbzDCoC"
      },
      "source": [
        "# Useful Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kPx4fK5DEOy"
      },
      "source": [
        "# create pytorch dataloader\n",
        "def create_dataloader(fold):\n",
        "\n",
        "    df = pd.read_csv(\"train_folds.csv\")\n",
        "    valid = df[df.kfold==fold].reset_index(drop=True)\n",
        "    valid[\"predictions\"] = 0\n",
        "    valid_electra_dataset = ElectraDataset(valid)\n",
        "    valid_electra_dataloader = torch.utils.data.DataLoader(valid_electra_dataset, batch_size= TEST_BATCH_SIZE)\n",
        "    valid_roberta_dataset = RobertaDataset(valid)\n",
        "    valid_roberta_dataloader = torch.utils.data.DataLoader(valid_roberta_dataset, batch_size= TEST_BATCH_SIZE)\n",
        "\n",
        "    return valid_electra_dataloader, valid_roberta_dataloader, valid[[\"predictions\", \"target\"]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zJZyIA_j7WK"
      },
      "source": [
        "def create_model(device, name, path):\n",
        "    \n",
        "    if name == \"RobertaLargeAttention\":\n",
        "\n",
        "        model = RobertaLargeAttention().to(device)\n",
        "        model.load_state_dict(torch.load(path))\n",
        "\n",
        "    elif name == \"RobertaLargeMean\":\n",
        "\n",
        "        model = RobertaLargeMean().to(device)\n",
        "        model.load_state_dict(torch.load(path))\n",
        "\n",
        "    elif name == \"ElectraLarge\":\n",
        "\n",
        "        model = ElectraLarge().to(device)\n",
        "        model.load_state_dict(torch.load(path))\n",
        "\n",
        "    elif name == \"RobertaBaseAttention\":\n",
        "\n",
        "        model = RobertaBaseAttention().to(device)\n",
        "        model.load_state_dict(torch.load(path))\n",
        "\n",
        "    else:\n",
        "        raise Exception(f\"Unknown model: {name}\")\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4nMpCnC8xKh"
      },
      "source": [
        "# Prepare Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t61FDt5R8w0M"
      },
      "source": [
        "def prepare_models():\n",
        "    device = torch.device(\"cuda\")\n",
        "    electra = [_ for i in range(5)]\n",
        "    robertalargemean = [_ for i in range(5)]\n",
        "    robertalargeattention = [_ for i in range(5)]\n",
        "    robertabase = [_ for i in range(5)]\n",
        "\n",
        "    for fold in range(3,5):\n",
        "        print(f\"Creating models for fold {fold} ...\")\n",
        "        electra[fold] = create_model(device, \"ElectraLarge\", f\"Models/CodeElectraLargeBaseline/model{fold}.bin\")\n",
        "        robertalargemean[fold] = create_model(device, \"RobertaLargeMean\", f\"Models/CodeRobertaLargeMean/model{fold}.bin\")\n",
        "        robertalargeattention[fold] = create_model(device, \"RobertaLargeAttention\", f\"Models/CodeRobertaLargeAttentionNorm2/model{fold}.bin\")\n",
        "        robertabase[fold] = create_model(device, \"RobertaBaseAttention\", f\"Models/CodeRobertaBaseAttentionNorm/model{fold}.bin\")\n",
        "\n",
        "    return electra, robertalargemean, robertalargeattention, robertabase, device"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s48iAtHFQN3a"
      },
      "source": [
        "# Get all our predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvIMYml4PVAf"
      },
      "source": [
        "def create_predictions(models, fold):\n",
        "    dataloader_electra, dataloader_roberta, valid_df = create_dataloader(fold)\n",
        "\n",
        "    for model in mymodels:\n",
        "      predictions = inf_fn(dataloader_electra, dataloader_roberta, model, device)\n",
        "      valid_df[str(model.__class__.__name__)] = predictions\n",
        "\n",
        "    return valid_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYgMtBpyQ-TS"
      },
      "source": [
        "# Run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kB8I4SfdRClW",
        "outputId": "83e05986-a1cd-4c01-f088-82bf1697dab2"
      },
      "source": [
        "# Create all the models for the different folds and for each model types \n",
        "electra, robertalargemean, robertalargeattention, robertabase, device = prepare_models()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating models for fold 3 ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias']\n",
            "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Creating models for fold 4 ...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at google/electra-large-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias']\n",
            "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.bias']\n",
            "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ey6ChodKTJfW"
      },
      "source": [
        "device = torch.device(\"cuda\")\n",
        "valids = [ _ for i in range(5)]\n",
        "for fold in range(3,5):\n",
        "\n",
        "  mymodels = [electra[fold], robertalargemean[fold], robertalargeattention[fold], robertabase[fold]]\n",
        "  valids[fold] = create_predictions(mymodels, fold)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SK1Ka4SccF2s",
        "outputId": "859efaf6-bb55-4406-856f-27f7deb12413"
      },
      "source": [
        "dataset = valids[3]\n",
        "for fold in range(3,5):\n",
        "  dataset = pd.concat([dataset,valids[fold]])\n",
        "\n",
        "print(f\"ElectraLarge : {mean_squared_error(dataset.target, dataset.ElectraLarge, squared=False)}\")\n",
        "print(f\"RobertaLargeMean : {mean_squared_error(dataset.target, dataset.RobertaLargeMean, squared=False)}\")\n",
        "print(f\"RobertaLargeAttention : {mean_squared_error(dataset.target, dataset.RobertaLargeAttention, squared=False)}\")\n",
        "print(f\"RobertaBaseAttention : {mean_squared_error(dataset.target, dataset.RobertaBaseAttention, squared=False)}\")\n",
        "\n",
        "dataset.to_csv(\"tuning2.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ElectraLarge : 0.4897325125403068\n",
            "RobertaLargeMean : 0.47503992885684787\n",
            "RobertaLargeAttention : 0.4810795474955003\n",
            "RobertaBaseAttention : 0.48282229526676057\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFrAGMbxUlkw"
      },
      "source": [
        "# Import the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zo60aMbtRjHJ",
        "outputId": "7b47fd1e-7b26-4373-cdc1-8b010930a27e"
      },
      "source": [
        "df = pd.read_csv(\"tuning1.csv\")\n",
        "df2 = pd.read_csv(\"tuning2.csv\")\n",
        "\n",
        "df = pd.concat([df,df2])\n",
        "\n",
        "print(df.shape)\n",
        "df = df.drop_duplicates()\n",
        "print(df.shape)\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "print(f\"ElectraLarge : {mean_squared_error(dataset.target, dataset.ElectraLarge, squared=False)}\")\n",
        "print(f\"RobertaLargeMean : {mean_squared_error(dataset.target, dataset.RobertaLargeMean, squared=False)}\")\n",
        "print(f\"RobertaLargeAttention : {mean_squared_error(dataset.target, dataset.RobertaLargeAttention, squared=False)}\")\n",
        "print(f\"RobertaBaseAttention : {mean_squared_error(dataset.target, dataset.RobertaBaseAttention, squared=False)}\")\n",
        "\n",
        "df.to_csv(\"tuning.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(3968, 7)\n",
            "(2834, 7)\n",
            "\n",
            "\n",
            "ElectraLarge : 0.4897325125403068\n",
            "RobertaLargeMean : 0.47503992885684787\n",
            "RobertaLargeAttention : 0.4810795474955003\n",
            "RobertaBaseAttention : 0.48282229526676057\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_GngTJj_MXu"
      },
      "source": [
        "# Optuna Tuning Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sY3KGVKZm_Vh",
        "outputId": "2f5a6f69-a528-4b5d-8cac-ff5abcd06a3d"
      },
      "source": [
        "pip install optuna"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-2.8.0-py3-none-any.whl (301 kB)\n",
            "\u001b[K     |████████████████████████████████| 301 kB 15.3 MB/s \n",
            "\u001b[?25hCollecting cliff\n",
            "  Downloading cliff-3.8.0-py3-none-any.whl (80 kB)\n",
            "\u001b[K     |████████████████████████████████| 80 kB 7.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from optuna) (1.19.5)\n",
            "Collecting colorlog\n",
            "  Downloading colorlog-5.0.1-py2.py3-none-any.whl (10 kB)\n",
            "Collecting cmaes>=0.8.2\n",
            "  Downloading cmaes-0.8.2-py3-none-any.whl (15 kB)\n",
            "Collecting alembic\n",
            "  Downloading alembic-1.6.5-py2.py3-none-any.whl (164 kB)\n",
            "\u001b[K     |████████████████████████████████| 164 kB 23.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (21.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from optuna) (4.41.1)\n",
            "Requirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.4.20)\n",
            "Requirement already satisfied: scipy!=1.4.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.4.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->optuna) (2.4.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna) (1.1.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna) (4.6.1)\n",
            "Collecting Mako\n",
            "  Downloading Mako-1.1.4-py2.py3-none-any.whl (75 kB)\n",
            "\u001b[K     |████████████████████████████████| 75 kB 3.8 MB/s \n",
            "\u001b[?25hCollecting python-editor>=0.3\n",
            "  Downloading python_editor-1.0.4-py3-none-any.whl (4.9 kB)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from alembic->optuna) (2.8.1)\n",
            "Collecting stevedore>=2.0.1\n",
            "  Downloading stevedore-3.3.0-py3-none-any.whl (49 kB)\n",
            "\u001b[K     |████████████████████████████████| 49 kB 5.2 MB/s \n",
            "\u001b[?25hCollecting pbr!=2.1.0,>=2.0.0\n",
            "  Downloading pbr-5.6.0-py2.py3-none-any.whl (111 kB)\n",
            "\u001b[K     |████████████████████████████████| 111 kB 34.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (2.1.0)\n",
            "Requirement already satisfied: PyYAML>=3.12 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (3.13)\n",
            "Collecting cmd2>=1.0.0\n",
            "  Downloading cmd2-2.1.2-py3-none-any.whl (141 kB)\n",
            "\u001b[K     |████████████████████████████████| 141 kB 22.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (0.2.5)\n",
            "Collecting colorama>=0.3.7\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Collecting pyperclip>=1.6\n",
            "  Downloading pyperclip-1.8.2.tar.gz (20 kB)\n",
            "Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (21.2.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->sqlalchemy>=1.1.0->optuna) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako->alembic->optuna) (2.0.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil->alembic->optuna) (1.15.0)\n",
            "Building wheels for collected packages: pyperclip\n",
            "  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyperclip: filename=pyperclip-1.8.2-py3-none-any.whl size=11137 sha256=ef3649bf4442f1add51b503e9e526d2f585b061d2f70324e1dfd72c5981a44c5\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/18/84/8f69f8b08169c7bae2dde6bd7daf0c19fca8c8e500ee620a28\n",
            "Successfully built pyperclip\n",
            "Installing collected packages: pyperclip, pbr, colorama, stevedore, python-editor, Mako, cmd2, colorlog, cmaes, cliff, alembic, optuna\n",
            "Successfully installed Mako-1.1.4 alembic-1.6.5 cliff-3.8.0 cmaes-0.8.2 cmd2-2.1.2 colorama-0.4.4 colorlog-5.0.1 optuna-2.8.0 pbr-5.6.0 pyperclip-1.8.2 python-editor-1.0.4 stevedore-3.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHz6y_cem8mq"
      },
      "source": [
        "import optuna\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ocFNtS7mjAz",
        "outputId": "5c9edda5-1053-416e-f38c-f05ad67bb667"
      },
      "source": [
        "%cd drive/MyDrive/CommonLit\n",
        "df = pd.read_csv(\"tuning.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/CommonLit\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D62eKJOW_Og0"
      },
      "source": [
        "def objective(trial,df = df):\n",
        "\n",
        "    r_min = 0\n",
        "    r_max = 1\n",
        "\n",
        "    a = trial.suggest_uniform('a', r_min, r_max)\n",
        "    b = trial.suggest_uniform('b', r_min, r_max)\n",
        "    c = trial.suggest_uniform('c', r_min, r_max)\n",
        "    d = trial.suggest_uniform('d', r_min, r_max)\n",
        "\n",
        "\n",
        "    df[\"predictions\"] = a * df[\"ElectraLarge\"] + b * df[\"RobertaLargeMean\"] + c * df[\"RobertaLargeAttention\"] + d * df[\"RobertaBaseAttention\"] \n",
        "\n",
        "\n",
        "    return mean_squared_error(df.target, df.predictions, squared=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcpplvcffxN7"
      },
      "source": [
        "# Optuna Blending"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZgj2B2mro2M",
        "outputId": "b717ff44-ebdc-4a61-93e0-41f227043c5b"
      },
      "source": [
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=100)\n",
        "print('Number of finished trials:', len(study.trials))\n",
        "print('Best trial:', study.best_trial.params)\n",
        "print('Best Score:', study.best_value)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2021-08-01 16:49:27,782]\u001b[0m A new study created in memory with name: no-name-17fc7957-9239-4909-8937-c67c0c1646eb\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:27,790]\u001b[0m Trial 0 finished with value: 2.9014326648196715 and parameters: {'a': 0.9850085718612486, 'b': 0.7935180485128943, 'c': 0.9437979298665712, 'd': 0.5062935762571645}. Best is trial 0 with value: 2.9014326648196715.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:27,795]\u001b[0m Trial 1 finished with value: 1.854555386263828 and parameters: {'a': 0.6336315690622176, 'b': 0.1512905136892917, 'c': 0.8300966125877, 'd': 0.7820130282678791}. Best is trial 1 with value: 1.854555386263828.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:27,803]\u001b[0m Trial 2 finished with value: 0.4750958480624098 and parameters: {'a': 0.03573373223063214, 'b': 0.11323692497135895, 'c': 0.07960684273683805, 'd': 0.7796845201859508}. Best is trial 2 with value: 0.4750958480624098.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:27,808]\u001b[0m Trial 3 finished with value: 0.6718657388614296 and parameters: {'a': 0.2742837375831285, 'b': 0.19980582996265794, 'c': 0.23185197533769042, 'd': 0.6828024002765299}. Best is trial 2 with value: 0.4750958480624098.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:27,814]\u001b[0m Trial 4 finished with value: 1.0665973607716082 and parameters: {'a': 0.9599935022953916, 'b': 0.13626046485957377, 'c': 0.10232321369615316, 'd': 0.555815454506466}. Best is trial 2 with value: 0.4750958480624098.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:27,820]\u001b[0m Trial 5 finished with value: 0.49573503476429426 and parameters: {'a': 0.10285466773675878, 'b': 0.39401950153628773, 'c': 0.39096766777713, 'd': 0.021632083543895786}. Best is trial 2 with value: 0.4750958480624098.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:27,828]\u001b[0m Trial 6 finished with value: 1.0636315256942743 and parameters: {'a': 0.4085476911984538, 'b': 0.5541210109838353, 'c': 0.23612401747306833, 'd': 0.5598582979011074}. Best is trial 2 with value: 0.4750958480624098.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:27,833]\u001b[0m Trial 7 finished with value: 0.7030016845248862 and parameters: {'a': 0.8049439732181836, 'b': 0.282480350168121, 'c': 0.12030507236503662, 'd': 0.21694286074965263}. Best is trial 2 with value: 0.4750958480624098.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:27,841]\u001b[0m Trial 8 finished with value: 1.3546759905567713 and parameters: {'a': 0.021700186870312832, 'b': 0.6721864541520177, 'c': 0.8011058668323795, 'd': 0.503344594167459}. Best is trial 2 with value: 0.4750958480624098.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:27,846]\u001b[0m Trial 9 finished with value: 0.8338382653903489 and parameters: {'a': 0.2526880878154739, 'b': 0.8189633743238792, 'c': 0.23463757218494408, 'd': 0.25140506957611775}. Best is trial 2 with value: 0.4750958480624098.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:27,858]\u001b[0m Trial 10 finished with value: 1.4006065567216641 and parameters: {'a': 0.524499057370776, 'b': 0.01692534485812125, 'c': 0.5829346633119041, 'd': 0.9044445884041183}. Best is trial 2 with value: 0.4750958480624098.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:27,874]\u001b[0m Trial 11 finished with value: 0.5216585676823285 and parameters: {'a': 0.0018965763504379882, 'b': 0.4048718334659224, 'c': 0.4454762426676835, 'd': 0.007169842037626365}. Best is trial 2 with value: 0.4750958480624098.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:27,888]\u001b[0m Trial 12 finished with value: 0.4741138355146102 and parameters: {'a': 0.1280135611967284, 'b': 0.40422619896107737, 'c': 0.4643598455451019, 'd': 0.0030820234965478165}. Best is trial 12 with value: 0.4741138355146102.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:27,901]\u001b[0m Trial 13 finished with value: 1.0576797917828025 and parameters: {'a': 0.171683752008854, 'b': 0.0035800199422228035, 'c': 0.5800444822818631, 'd': 0.9830896503538176}. Best is trial 12 with value: 0.4741138355146102.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:27,913]\u001b[0m Trial 14 finished with value: 0.5008198703871087 and parameters: {'a': 0.00515268365114302, 'b': 0.5782262256386336, 'c': 0.0032952864910058555, 'd': 0.30838959738870475}. Best is trial 12 with value: 0.4741138355146102.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:27,925]\u001b[0m Trial 15 finished with value: 2.366575472699931 and parameters: {'a': 0.36239596486504294, 'b': 0.9539630425598584, 'c': 0.7040027942805835, 'd': 0.7877579639448439}. Best is trial 12 with value: 0.4741138355146102.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:27,939]\u001b[0m Trial 16 finished with value: 0.484754141167842 and parameters: {'a': 0.11471581122957121, 'b': 0.3423148490296269, 'c': 0.3425480347458941, 'd': 0.13232732426487923}. Best is trial 12 with value: 0.4741138355146102.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:27,951]\u001b[0m Trial 17 finished with value: 0.47104318651543997 and parameters: {'a': 0.21345011136132055, 'b': 0.4691062209960733, 'c': 0.013345601842077517, 'd': 0.3718608781618326}. Best is trial 17 with value: 0.47104318651543997.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:27,963]\u001b[0m Trial 18 finished with value: 1.468146169144016 and parameters: {'a': 0.23443741328403844, 'b': 0.47739129829431703, 'c': 0.9898686793539423, 'd': 0.38979077972479537}. Best is trial 17 with value: 0.47104318651543997.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:27,974]\u001b[0m Trial 19 finished with value: 1.2574519779077067 and parameters: {'a': 0.3558465635998502, 'b': 0.6495251565463761, 'c': 0.5413381659197134, 'd': 0.3764767269806737}. Best is trial 17 with value: 0.47104318651543997.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:27,988]\u001b[0m Trial 20 finished with value: 1.1886721966208267 and parameters: {'a': 0.5560485232547696, 'b': 0.49079405365454803, 'c': 0.6648879660768859, 'd': 0.15424569036527547}. Best is trial 17 with value: 0.47104318651543997.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:28,006]\u001b[0m Trial 21 finished with value: 0.471257333184442 and parameters: {'a': 0.09503415308304894, 'b': 0.24571289475037886, 'c': 0.015153314830991355, 'd': 0.6531631081976841}. Best is trial 17 with value: 0.47104318651543997.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:28,022]\u001b[0m Trial 22 finished with value: 0.4816150144676408 and parameters: {'a': 0.15996737521013177, 'b': 0.2824981461085707, 'c': 0.027487381231679833, 'd': 0.6316452175439871}. Best is trial 17 with value: 0.47104318651543997.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:28,037]\u001b[0m Trial 23 finished with value: 0.4807800610342339 and parameters: {'a': 0.10484516391233, 'b': 0.25484958279644354, 'c': 0.17734221824686852, 'd': 0.3990189258639275}. Best is trial 17 with value: 0.47104318651543997.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:28,049]\u001b[0m Trial 24 finished with value: 0.671198529903285 and parameters: {'a': 0.29217131234778754, 'b': 0.4127647603438196, 'c': 0.002412365035642794, 'd': 0.6826092837235646}. Best is trial 17 with value: 0.47104318651543997.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:28,063]\u001b[0m Trial 25 finished with value: 0.47612221796662707 and parameters: {'a': 0.19387759936219673, 'b': 0.3444514566781716, 'c': 0.34347130187171726, 'd': 0.08196532975045617}. Best is trial 17 with value: 0.47104318651543997.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:28,077]\u001b[0m Trial 26 finished with value: 0.9932274883412752 and parameters: {'a': 0.0752310850408145, 'b': 0.5710851438044463, 'c': 0.1529594097370674, 'd': 0.8909097687622913}. Best is trial 17 with value: 0.47104318651543997.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:28,091]\u001b[0m Trial 27 finished with value: 1.1639828319271226 and parameters: {'a': 0.426574859203697, 'b': 0.4728019271855479, 'c': 0.30029478757365335, 'd': 0.6423736706450489}. Best is trial 17 with value: 0.47104318651543997.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:28,104]\u001b[0m Trial 28 finished with value: 0.500114473377114 and parameters: {'a': 0.32064279477204827, 'b': 0.061967717093919905, 'c': 0.48153922295374446, 'd': 0.2933610690313835}. Best is trial 17 with value: 0.47104318651543997.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:28,119]\u001b[0m Trial 29 finished with value: 0.6438056775994507 and parameters: {'a': 0.19124619934032747, 'b': 0.712030510814422, 'c': 0.04136100905814427, 'd': 0.419985602779408}. Best is trial 17 with value: 0.47104318651543997.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:28,133]\u001b[0m Trial 30 finished with value: 1.4278613213614844 and parameters: {'a': 0.44975852167504987, 'b': 0.23632986220730978, 'c': 0.8856764421420052, 'd': 0.48576107861005935}. Best is trial 17 with value: 0.47104318651543997.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:28,146]\u001b[0m Trial 31 finished with value: 0.47514686260013994 and parameters: {'a': 0.040947964143950494, 'b': 0.10315586765429358, 'c': 0.08637098296236415, 'd': 0.7691803462772946}. Best is trial 17 with value: 0.47104318651543997.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:28,162]\u001b[0m Trial 32 finished with value: 0.4983423327189708 and parameters: {'a': 0.06084160829308219, 'b': 0.17676797946056425, 'c': 0.04912591970370822, 'd': 0.8433335034966799}. Best is trial 17 with value: 0.47104318651543997.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:28,177]\u001b[0m Trial 33 finished with value: 0.9709720800693655 and parameters: {'a': 0.14831547756905783, 'b': 0.34604872633788875, 'c': 0.18286639608220517, 'd': 0.9900336113653782}. Best is trial 17 with value: 0.47104318651543997.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:28,191]\u001b[0m Trial 34 finished with value: 0.5571242979651321 and parameters: {'a': 0.2306238495936759, 'b': 0.1997061226853528, 'c': 0.0827951689666361, 'd': 0.7342543179711191}. Best is trial 17 with value: 0.47104318651543997.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:28,205]\u001b[0m Trial 35 finished with value: 0.8649164792874972 and parameters: {'a': 0.6100127617339306, 'b': 0.10552614321195287, 'c': 0.13488265948798356, 'd': 0.7255261330856931}. Best is trial 17 with value: 0.47104318651543997.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:28,218]\u001b[0m Trial 36 finished with value: 0.47211044487400944 and parameters: {'a': 0.004853724801633513, 'b': 0.4109133440210687, 'c': 0.004589913755444958, 'd': 0.591875538852897}. Best is trial 17 with value: 0.47104318651543997.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:28,230]\u001b[0m Trial 37 finished with value: 0.49663982106176463 and parameters: {'a': 0.12678996462819042, 'b': 0.4314380662776332, 'c': 0.006511394248524958, 'd': 0.5809260856342613}. Best is trial 17 with value: 0.47104318651543997.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:28,243]\u001b[0m Trial 38 finished with value: 1.3863781034161193 and parameters: {'a': 0.7491055033048344, 'b': 0.5239136622595071, 'c': 0.27916931125075944, 'd': 0.4761381406019757}. Best is trial 17 with value: 0.47104318651543997.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:28,255]\u001b[0m Trial 39 finished with value: 0.9694917469891509 and parameters: {'a': 0.00025674525278937543, 'b': 0.322949877504976, 'c': 0.7443551719600427, 'd': 0.6011588239448677}. Best is trial 17 with value: 0.47104318651543997.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:28,271]\u001b[0m Trial 40 finished with value: 0.4983184708694798 and parameters: {'a': 0.0669535575230473, 'b': 0.4463376002289109, 'c': 0.18975128148313397, 'd': 0.4484324465252686}. Best is trial 17 with value: 0.47104318651543997.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:28,284]\u001b[0m Trial 41 finished with value: 0.4718641482029385 and parameters: {'a': 0.04942824057057979, 'b': 0.37778334397081204, 'c': 0.06862515288094302, 'd': 0.5547915502365106}. Best is trial 17 with value: 0.47104318651543997.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:28,297]\u001b[0m Trial 42 finished with value: 0.4921407335110017 and parameters: {'a': 0.002522947262103406, 'b': 0.3775395282770255, 'c': 0.0717425591134527, 'd': 0.6726064253062409}. Best is trial 17 with value: 0.47104318651543997.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:28,310]\u001b[0m Trial 43 finished with value: 0.6475546112555715 and parameters: {'a': 0.07409213621042389, 'b': 0.6089321444593812, 'c': 0.11970346262173565, 'd': 0.5622611770517022}. Best is trial 17 with value: 0.47104318651543997.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:28,324]\u001b[0m Trial 44 finished with value: 0.46903341722822517 and parameters: {'a': 0.21464520012677157, 'b': 0.2971846003878891, 'c': 0.0014734328437158162, 'd': 0.5308935545770492}. Best is trial 44 with value: 0.46903341722822517.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:28,337]\u001b[0m Trial 45 finished with value: 0.46853148303912645 and parameters: {'a': 0.2137294657149327, 'b': 0.29706215880825776, 'c': 0.0025193345045270246, 'd': 0.524825217061373}. Best is trial 45 with value: 0.46853148303912645.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:28,349]\u001b[0m Trial 46 finished with value: 0.4682944614427437 and parameters: {'a': 0.21297989420598193, 'b': 0.2895076050189023, 'c': 0.0002121143792110508, 'd': 0.5023775021072708}. Best is trial 46 with value: 0.4682944614427437.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:28,362]\u001b[0m Trial 47 finished with value: 0.4784009486319165 and parameters: {'a': 0.2875703043969775, 'b': 0.28158689867782477, 'c': 0.008082696850569105, 'd': 0.5217874033212846}. Best is trial 46 with value: 0.4682944614427437.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:28,374]\u001b[0m Trial 48 finished with value: 0.5283335635245482 and parameters: {'a': 0.20695391092582152, 'b': 0.15953318417989903, 'c': 0.11338745171141584, 'd': 0.3509042544424555}. Best is trial 46 with value: 0.4682944614427437.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:28,386]\u001b[0m Trial 49 finished with value: 0.5628030063806596 and parameters: {'a': 0.3420520106989745, 'b': 0.2333347874797761, 'c': 0.23258394669499652, 'd': 0.45577289110871133}. Best is trial 46 with value: 0.4682944614427437.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:28,399]\u001b[0m Trial 50 finished with value: 0.5689208672367825 and parameters: {'a': 0.38832135008396795, 'b': 0.3058007355363352, 'c': 0.05312379735862388, 'd': 0.5240962845524719}. Best is trial 46 with value: 0.4682944614427437.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:28,413]\u001b[0m Trial 51 finished with value: 0.52898763014706 and parameters: {'a': 0.25397767441537666, 'b': 0.37925697413648485, 'c': 0.045910298783278444, 'd': 0.532999138936888}. Best is trial 46 with value: 0.4682944614427437.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:28,426]\u001b[0m Trial 52 finished with value: 0.5481034089988196 and parameters: {'a': 0.3021995710197912, 'b': 0.21386674590894306, 'c': 0.0930248891313107, 'd': 0.6293343701603965}. Best is trial 46 with value: 0.4682944614427437.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:28,439]\u001b[0m Trial 53 finished with value: 0.5122285382914938 and parameters: {'a': 0.1659443605571407, 'b': 0.2713429204545004, 'c': 0.001034592077394307, 'd': 0.4225314728863436}. Best is trial 46 with value: 0.4682944614427437.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:28,453]\u001b[0m Trial 54 finished with value: 0.5243075620354417 and parameters: {'a': 0.24438011400427243, 'b': 0.31030118023061665, 'c': 0.14861756416569744, 'd': 0.5014696307066312}. Best is trial 46 with value: 0.4682944614427437.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:28,468]\u001b[0m Trial 55 finished with value: 0.512677630469319 and parameters: {'a': 0.10922379027894279, 'b': 0.374356249134028, 'c': 0.04259518395259506, 'd': 0.3361893314046648}. Best is trial 46 with value: 0.4682944614427437.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:28,482]\u001b[0m Trial 56 finished with value: 0.6454833480633272 and parameters: {'a': 0.2096270457897597, 'b': 0.5263654943800815, 'c': 0.07841929982005333, 'd': 0.5501581551015172}. Best is trial 46 with value: 0.4682944614427437.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:28,497]\u001b[0m Trial 57 finished with value: 0.46849212608640334 and parameters: {'a': 0.14282661584559198, 'b': 0.4511267254976006, 'c': 0.20136195572148183, 'd': 0.2248225371199037}. Best is trial 46 with value: 0.4682944614427437.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:28,512]\u001b[0m Trial 58 finished with value: 0.4858112298264572 and parameters: {'a': 0.27069635102945744, 'b': 0.46219723295827375, 'c': 0.20119589610729038, 'd': 0.19254203279148185}. Best is trial 46 with value: 0.4682944614427437.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:28,524]\u001b[0m Trial 59 finished with value: 0.6214014822278602 and parameters: {'a': 0.14154357122655986, 'b': 0.5191061782998049, 'c': 0.42259017801375554, 'd': 0.25455635017016964}. Best is trial 46 with value: 0.4682944614427437.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:28,536]\u001b[0m Trial 60 finished with value: 0.46902992784039504 and parameters: {'a': 0.17720255141082478, 'b': 0.24749572506014955, 'c': 0.27503652613058693, 'd': 0.28962737141169814}. Best is trial 46 with value: 0.4682944614427437.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:28,549]\u001b[0m Trial 61 finished with value: 0.47010989919931545 and parameters: {'a': 0.17751660057679644, 'b': 0.18142533884502426, 'c': 0.35578324942919914, 'd': 0.26845740894051034}. Best is trial 46 with value: 0.4682944614427437.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:28,562]\u001b[0m Trial 62 finished with value: 0.47860438136318945 and parameters: {'a': 0.16922779557647943, 'b': 0.1358255858620912, 'c': 0.38113259235691055, 'd': 0.2571353412968952}. Best is trial 46 with value: 0.4682944614427437.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:28,577]\u001b[0m Trial 63 finished with value: 0.48291709112558373 and parameters: {'a': 0.22555645067886346, 'b': 0.2020704334430378, 'c': 0.29316095584818463, 'd': 0.20770031916182016}. Best is trial 46 with value: 0.4682944614427437.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:28,591]\u001b[0m Trial 64 finished with value: 0.4786551884556337 and parameters: {'a': 0.32128358167423, 'b': 0.29386254239317966, 'c': 0.32258472972168795, 'd': 0.16601055966960798}. Best is trial 46 with value: 0.4682944614427437.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:28,607]\u001b[0m Trial 65 finished with value: 0.47406651441534425 and parameters: {'a': 0.18984493383361645, 'b': 0.06434919119893032, 'c': 0.40592806267923565, 'd': 0.3005765199527352}. Best is trial 46 with value: 0.4682944614427437.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:28,622]\u001b[0m Trial 66 finished with value: 0.9270384044676985 and parameters: {'a': 0.9706763779277203, 'b': 0.349597055881863, 'c': 0.24539846624436096, 'd': 0.07156027257492337}. Best is trial 46 with value: 0.4682944614427437.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:28,637]\u001b[0m Trial 67 finished with value: 0.4867875696599939 and parameters: {'a': 0.25716028636565125, 'b': 0.18078504842409385, 'c': 0.36425676963645215, 'd': 0.3258875692751466}. Best is trial 46 with value: 0.4682944614427437.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:28,651]\u001b[0m Trial 68 finished with value: 0.5336804366284026 and parameters: {'a': 0.13417700395567383, 'b': 0.2603698941231837, 'c': 0.5511090479956221, 'd': 0.2714941694827274}. Best is trial 46 with value: 0.4682944614427437.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:28,665]\u001b[0m Trial 69 finished with value: 0.5139574804871426 and parameters: {'a': 0.18676523590531366, 'b': 0.14847543986163694, 'c': 0.1520795549829015, 'd': 0.36822752423781885}. Best is trial 46 with value: 0.4682944614427437.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:28,680]\u001b[0m Trial 70 finished with value: 0.4772926570494449 and parameters: {'a': 0.37153302076028305, 'b': 0.22284524454937987, 'c': 0.2778787549271499, 'd': 0.22810813355741963}. Best is trial 46 with value: 0.4682944614427437.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:28,694]\u001b[0m Trial 71 finished with value: 0.4682802145936613 and parameters: {'a': 0.09530449530902342, 'b': 0.2555037676578627, 'c': 0.26542147124656085, 'd': 0.41232506187325385}. Best is trial 71 with value: 0.4682802145936613.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:28,707]\u001b[0m Trial 72 finished with value: 0.4688171147324584 and parameters: {'a': 0.09533731879995687, 'b': 0.2595282061446974, 'c': 0.261072785800978, 'd': 0.4233113875567387}. Best is trial 71 with value: 0.4682802145936613.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:28,721]\u001b[0m Trial 73 finished with value: 0.5122250639382384 and parameters: {'a': 0.09138210834432445, 'b': 0.32246588605092086, 'c': 0.3376504136312382, 'd': 0.11611452377462524}. Best is trial 71 with value: 0.4682802145936613.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:28,734]\u001b[0m Trial 74 finished with value: 0.49534778980935484 and parameters: {'a': 0.029789369411334032, 'b': 0.25061628381224504, 'c': 0.21617568254172953, 'd': 0.40212279604325823}. Best is trial 71 with value: 0.4682802145936613.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:28,747]\u001b[0m Trial 75 finished with value: 0.47005552084539903 and parameters: {'a': 0.10723472755811296, 'b': 0.1776608451258728, 'c': 0.2475865310893229, 'd': 0.4553476439382843}. Best is trial 71 with value: 0.4682802145936613.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:28,760]\u001b[0m Trial 76 finished with value: 0.5127621273315718 and parameters: {'a': 0.11607639428745384, 'b': 0.04193420832882694, 'c': 0.2575180877516657, 'd': 0.44278955826235095}. Best is trial 71 with value: 0.4682802145936613.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:28,773]\u001b[0m Trial 77 finished with value: 0.46930813860147164 and parameters: {'a': 0.08879544692742224, 'b': 0.13163901908821848, 'c': 0.3105417435209102, 'd': 0.4727347248883169}. Best is trial 71 with value: 0.4682802145936613.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:28,789]\u001b[0m Trial 78 finished with value: 0.4953236740521848 and parameters: {'a': 0.143162944274373, 'b': 0.11024918088534683, 'c': 0.16747946989437001, 'd': 0.47362994038550904}. Best is trial 71 with value: 0.4682802145936613.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:28,804]\u001b[0m Trial 79 finished with value: 0.5049456509410962 and parameters: {'a': 0.08337144814170501, 'b': 0.339897982680786, 'c': 0.31581375210003043, 'd': 0.42687917201135067}. Best is trial 71 with value: 0.4682802145936613.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:28,819]\u001b[0m Trial 80 finished with value: 0.4766853567980999 and parameters: {'a': 0.04229749715882885, 'b': 0.0780024632954871, 'c': 0.45729970812540927, 'd': 0.49730903087319905}. Best is trial 71 with value: 0.4682802145936613.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:28,833]\u001b[0m Trial 81 finished with value: 0.4730297830635969 and parameters: {'a': 0.10483047848974321, 'b': 0.1309361351572548, 'c': 0.26499110165569123, 'd': 0.4681195990680115}. Best is trial 71 with value: 0.4682802145936613.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:28,846]\u001b[0m Trial 82 finished with value: 0.558445951683142 and parameters: {'a': 0.15188814433478953, 'b': 0.2742351739079871, 'c': 0.2151117733555496, 'd': 0.6116117076888768}. Best is trial 71 with value: 0.4682802145936613.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:28,859]\u001b[0m Trial 83 finished with value: 0.48167710025457355 and parameters: {'a': 0.22014531399023368, 'b': 0.20825539293358517, 'c': 0.28762122187516576, 'd': 0.3968650515072974}. Best is trial 71 with value: 0.4682802145936613.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:28,873]\u001b[0m Trial 84 finished with value: 0.4886388996920513 and parameters: {'a': 0.08058717324934678, 'b': 0.24041614601080588, 'c': 0.24173330177963873, 'd': 0.35106284934233567}. Best is trial 71 with value: 0.4682802145936613.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:28,887]\u001b[0m Trial 85 finished with value: 0.4838456775971816 and parameters: {'a': 0.055987802659937316, 'b': 0.16859088904380085, 'c': 0.3085413983438378, 'd': 0.5725800169786494}. Best is trial 71 with value: 0.4682802145936613.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:28,901]\u001b[0m Trial 86 finished with value: 0.4701457497638992 and parameters: {'a': 0.023593241974421142, 'b': 0.2869994371107206, 'c': 0.19420765264440412, 'd': 0.5182861806359648}. Best is trial 71 with value: 0.4682802145936613.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:28,917]\u001b[0m Trial 87 finished with value: 0.47408017278723946 and parameters: {'a': 0.12882738136997285, 'b': 0.01082157097690839, 'c': 0.3834741108239432, 'd': 0.5434524512196475}. Best is trial 71 with value: 0.4682802145936613.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:28,932]\u001b[0m Trial 88 finished with value: 0.46852643062973615 and parameters: {'a': 0.2039634751076629, 'b': 0.18392840939076266, 'c': 0.22363032927234375, 'd': 0.4388490513633672}. Best is trial 71 with value: 0.4682802145936613.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:28,947]\u001b[0m Trial 89 finished with value: 0.6118301282621978 and parameters: {'a': 0.47889825830264887, 'b': 0.22178061422007633, 'c': 0.13077966800284022, 'd': 0.4936844056653633}. Best is trial 71 with value: 0.4682802145936613.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:28,962]\u001b[0m Trial 90 finished with value: 0.48279484385415444 and parameters: {'a': 0.2741941922570707, 'b': 0.31148754784270616, 'c': 0.10436676954253538, 'd': 0.4277273265259707}. Best is trial 71 with value: 0.4682802145936613.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:28,976]\u001b[0m Trial 91 finished with value: 0.4676331389980447 and parameters: {'a': 0.1654968128943937, 'b': 0.18849169201290306, 'c': 0.22798289283669515, 'd': 0.450639158367492}. Best is trial 91 with value: 0.4676331389980447.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:28,990]\u001b[0m Trial 92 finished with value: 0.5233589035935958 and parameters: {'a': 0.16890437673471131, 'b': 0.08776215984835628, 'c': 0.21435759318877642, 'd': 0.3676599237487784}. Best is trial 91 with value: 0.4676331389980447.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:29,006]\u001b[0m Trial 93 finished with value: 0.47709781211915486 and parameters: {'a': 0.23219828341051124, 'b': 0.19088945249488387, 'c': 0.2654212016459768, 'd': 0.4089878655157028}. Best is trial 91 with value: 0.4676331389980447.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:29,020]\u001b[0m Trial 94 finished with value: 1.3356946018476803 and parameters: {'a': 0.19729455057215306, 'b': 0.9777265439459755, 'c': 0.33410685010189434, 'd': 0.47927297982566863}. Best is trial 91 with value: 0.4676331389980447.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:29,035]\u001b[0m Trial 95 finished with value: 0.49376811056892117 and parameters: {'a': 0.15182355017137922, 'b': 0.13288767535155777, 'c': 0.17141209778445565, 'd': 0.44169244196574614}. Best is trial 91 with value: 0.4676331389980447.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:29,048]\u001b[0m Trial 96 finished with value: 0.5032407682210026 and parameters: {'a': 0.308539787601096, 'b': 0.2534558921636821, 'c': 0.2214025753638346, 'd': 0.3845821590666356}. Best is trial 91 with value: 0.4676331389980447.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:29,064]\u001b[0m Trial 97 finished with value: 0.589807186353628 and parameters: {'a': 0.2040055991081871, 'b': 0.2951011443670943, 'c': 0.28961888244789924, 'd': 0.5089275452716897}. Best is trial 91 with value: 0.4676331389980447.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:29,079]\u001b[0m Trial 98 finished with value: 0.6102205640935725 and parameters: {'a': 0.16582475467554336, 'b': 0.3580532635765787, 'c': 0.5130762685590766, 'd': 0.2864337557878471}. Best is trial 91 with value: 0.4676331389980447.\u001b[0m\n",
            "\u001b[32m[I 2021-08-01 16:49:29,094]\u001b[0m Trial 99 finished with value: 1.1382206352522153 and parameters: {'a': 0.24109293619804542, 'b': 0.8419135346636039, 'c': 0.2007042923739431, 'd': 0.5395758292370311}. Best is trial 91 with value: 0.4676331389980447.\u001b[0m\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number of finished trials: 100\n",
            "Best trial: {'a': 0.1654968128943937, 'b': 0.18849169201290306, 'c': 0.22798289283669515, 'd': 0.450639158367492}\n",
            "Best Score: 0.4676331389980447\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p03cKgA-PePf"
      },
      "source": [
        "# Best trial: {'a': 0.1654968128943937, 'b': 0.18849169201290306, 'c': 0.22798289283669515, 'd': 0.450639158367492}\n",
        "# Best Score: 0.4676331389980447 LB ?"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxajVmxGcJRq"
      },
      "source": [
        "# ElectraLarge : 0.4897325125403068\n",
        "# RobertaLargeMean : 0.47503992885684787\n",
        "# RobertaLargeAttention : 0.4810795474955003\n",
        "# RobertaBaseAttention : 0.48282229526676057"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}